@article{EPCA,
  title={A Generalization of Principal Components Analysis to the Exponential Family},
  author={Collins, Michael and Dasgupta, Sanjoy and Schapire, Robert E},
  journal={Advances in Neural Information Processing Systems},
  volume={14},
  year={2001}
}

@article{azoury,
  title={Relative Loss Bounds for On-Line Density Estimation with the Exponential Family of Distributions},
  author={Azoury, Katy S and Warmuth, Manfred K},
  journal={Machine learning},
  volume={43},
  pages={211--246},
  year={2001},
  publisher={Springer}
}

@BOOK{GLM,
  title     = "Generalized Linear Models",
  author    = "McCullagh, P and Nelder, John A",
  publisher = "Chapman \& Hall/CRC",
  series    = "Chapman \& Hall/CRC Monographs on Statistics and Applied
               Probability",
  edition   =  2,
  month     =  aug,
  year      =  1989,
  address   = "Philadelphia, PA",
  language  = "en"
}


@article{PCA,
author = {Karl Pearson},
title = {On Lines and Planes of Closest Fit to Systems of Points in Space},
journal = {Philosophical Magazine},
volume = {2},
number = {11},
pages = {559--572},
year = {1901},
publisher = {Taylor \& Francis},
doi = {10.1080/14786440109462720},
}

@article{Bregman,
title = {The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming},
journal = {USSR Computational Mathematics and Mathematical Physics},
volume = {7},
number = {3},
pages = {200-217},
year = {1967},
issn = {0041-5553},
doi = {10.1016/0041-5553(67)90040-7},
url = {https://www.sciencedirect.com/science/article/pii/0041555367900407},
author = {L.M. Bregman},
abstract = {IN this paper we consider an iterative method of finding the common point of convex sets. This method can be regarded as a generalization of the methods discussed in [1–4]. Apart from problems which can be reduced to finding some point of the intersection of convex sets, the method considered can be applied to the approximate solution of problems in linear and convex programming.}
}

@ARTICLE{BregmanMotivation,
  author={Banerjee, A. and Xin Guo and Hui Wang},
  journal={IEEE Transactions on Information Theory}, 
  title={On the optimality of conditional expectation as a Bregman predictor}, 
  year={2005},
  volume={51},
  number={7},
  pages={2664-2669},
  keywords={Random variables;Sufficient conditions;Information theory;Operations research;Industrial engineering;Mathematics;Bregman loss functions (BLFs);conditional expectation;prediction},
  doi={10.1109/TIT.2005.850145}}

@article{Forester,
title = {Relative Expected Instantaneous Loss Bounds},
journal = {Journal of Computer and System Sciences},
volume = {64},
number = {1},
pages = {76-102},
year = {2002},
issn = {0022-0000},
doi = {10.1006/jcss.2001.1798},
url = {https://www.sciencedirect.com/science/article/pii/S0022000001917982},
author = {Jürgen Forster and Manfred K. Warmuth},
abstract = {In the literature a number of relative loss bounds have been shown for on-line learning algorithms. Here the relative loss is the total loss of the on-line algorithm in all trials minus the total loss of the best comparator that is chosen off-line. However, for many applications instantaneous loss bounds are more interesting where the learner first sees a batch of examples and then uses these examples to make a prediction on a new instance. We show relative expected instantaneous loss bounds for the case when the examples are i.i.d. with an unknown distribution. We bound the expected loss of the algorithm on the last example minus the expected loss of the best comparator on a random example. In particular, we study linear regression and density estimation problems and show how the leave-one-out loss can be used to prove instantaneous loss bounds for these cases. For linear regression we use an algorithm that is similar to a new on-line learning algorithm developed by Vovk. Recently a large number of relative total loss bounds have been shown that have the form O(lnT), where T is the number of trials/examples. Standard conversions of on-line algorithms to batch algorithms result in relative expected instantaneous loss bounds of the form O(lnTT). Our methods lead to O(1T) upper bounds. In many cases we give tight lower bounds.}
}