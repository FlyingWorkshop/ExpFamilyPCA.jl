<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Math · ExpFamilyPCA</title><meta name="title" content="Math · ExpFamilyPCA"/><meta property="og:title" content="Math · ExpFamilyPCA"/><meta property="twitter:title" content="Math · ExpFamilyPCA"/><meta name="description" content="Documentation for ExpFamilyPCA."/><meta property="og:description" content="Documentation for ExpFamilyPCA."/><meta property="twitter:description" content="Documentation for ExpFamilyPCA."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">ExpFamilyPCA</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">ExpFamilyPCA.jl</a></li><li class="is-active"><a class="tocitem" href>Math</a><ul class="internal"><li><a class="tocitem" href="#Principal-Component-Analysis-(PCA)"><span>Principal Component Analysis (PCA)</span></a></li><li><a class="tocitem" href="#Exponential-Family-PCA-(EPCA)"><span>Exponential Family PCA (EPCA)</span></a></li><li class="toplevel"><a class="tocitem" href="#References"><span>References</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Math</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Math</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/FlyingWorkshop/ExpFamilyPCA.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/FlyingWorkshop/ExpFamilyPCA.jl/blob/main/docs/src/math.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Math-Details"><a class="docs-heading-anchor" href="#Math-Details">Math Details</a><a id="Math-Details-1"></a><a class="docs-heading-anchor-permalink" href="#Math-Details" title="Permalink"></a></h1><p>The goal of this page is to introduce and motivate exponential family principal component analysis (EPCA) (<a href="#EPCA">Collins <em>et al.</em>, 2001</a>). This guide is accessible to anyone with knowledge of basic multivariable calculus and linear algebra (e.g., gradients, matrix rank).<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup> To ensure that readers can follow the math, we will write out every step and claim in exhaustive detail. We invite readers seeking a more concise, formal presentation of EPCA to explore the <a href="#EPCA">original paper</a>.</p><h2 id="Principal-Component-Analysis-(PCA)"><a class="docs-heading-anchor" href="#Principal-Component-Analysis-(PCA)">Principal Component Analysis (PCA)</a><a id="Principal-Component-Analysis-(PCA)-1"></a><a class="docs-heading-anchor-permalink" href="#Principal-Component-Analysis-(PCA)" title="Permalink"></a></h2><p>Principal component analysis (<a href="#PCA">Pearson, 1901</a>) is an extremely popular dimensionality reduction technique. It has been invented by several researchers throughout its history and appears across many fields. There are many interpretations and derivations of PCA, but we will only discuss two here. The first is PCA as a low-rank matrix approximation problem; the second is as a Gaussian denoising problem. </p><p>PCA (<a href="#PCA">Pearson, 1901</a>) is a powerful dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional subspace while retaining as much variability as possible. It accomplishes this by identifying the directions of maximum variance in the data, known as principal components, and projecting the data onto these new orthogonal axes. PCA finds extensive applications in various domains, including data visualization, noise reduction, feature extraction, and exploratory data analysis.</p><h3 id="Low-Rank-Matrix-Approximation"><a class="docs-heading-anchor" href="#Low-Rank-Matrix-Approximation">Low-Rank Matrix Approximation</a><a id="Low-Rank-Matrix-Approximation-1"></a><a class="docs-heading-anchor-permalink" href="#Low-Rank-Matrix-Approximation" title="Permalink"></a></h3><p>PCA can be formulated as an low-rank matrix approximation problem. For a data matrix <span>$X$</span>, we want to find low-dimensional approximation <span>$\Theta$</span> that minimizes the the sum of the squared Euclidean distances. Formally, we write</p><p class="math-container">\[\begin{aligned}
&amp; \underset{\Theta}{\text{minimize}}
&amp; &amp; \|X - \Theta\|_F \\
&amp; \text{subject to}
&amp; &amp; \mathrm{rank}\left(\Theta\right) \leq \ell
\end{aligned}\]</p><p>where <span>$\| \cdot \|_F$</span> is the Frobenius norm. Observe that the objective is equivalent to maximizing the log likelihood of a Gaussian model. Consequently, PCA can be viewed as a denoising procedure that recovers the true low-dimensional signal <span>$\Theta$</span> from a normally noised high-dimensional measurement <span>$X$</span>. </p><h3 id="Gaussian-Denoising"><a class="docs-heading-anchor" href="#Gaussian-Denoising">Gaussian Denoising</a><a id="Gaussian-Denoising-1"></a><a class="docs-heading-anchor-permalink" href="#Gaussian-Denoising" title="Permalink"></a></h3><p>TODO: include image from presentation</p><h2 id="Exponential-Family-PCA-(EPCA)"><a class="docs-heading-anchor" href="#Exponential-Family-PCA-(EPCA)">Exponential Family PCA (EPCA)</a><a id="Exponential-Family-PCA-(EPCA)-1"></a><a class="docs-heading-anchor-permalink" href="#Exponential-Family-PCA-(EPCA)" title="Permalink"></a></h2><p>EPCA is an extension of PCA analogous to how generalized linear models (<a href="#GLM">McCullagh and Nelder, 1989</a>) extend linear regression. In particular, EPCA can denoise from any exponential family. <a href="#EPCA">Collins <em>et al.</em> (2001)</a> showed that maximizing the log likelihood of any exponential family is directly related to minimizing the Bregman divergence</p><p class="math-container">\[\begin{aligned} 
B_F(p \| q) \equiv F(p) - F(q) - f(q)(p - q) 
\end{aligned}\]</p><p>where </p><p class="math-container">\[\begin{aligned}
    f(\mu) &amp;\equiv \nabla_\mu F(\mu) \\
    F(\mu) &amp;\equiv \theta \cdot g(\theta) - G(\theta)
\end{aligned}\]</p><p>and <span>$g$</span> is the link function, <span>$g(\theta) = \nabla_\theta G(\theta)$</span>, and <span>$\mu = g(\theta)$</span>. In other words, <span>$F$</span> is the convex dual of cumulant \citep{azoury2001relative}. We can now express the general formulation of the EPCA problem. For any differentiable convex function <span>$G$</span>, the EPCA problem is</p><p class="math-container">\[\begin{aligned}
&amp; \underset{\Theta}{\text{minimize}}
&amp; &amp; B_F\left(X \| g(\Theta) \right)
\end{aligned}\]</p><p>where <span>$g$</span> is applied elementwise across <span>$\Theta$</span> and <span>$B_F$</span> is the generalized Bregman divergence. Unfortunately, the optimal convergence constraints of the general problem remain unsolved. As such, in practice we minimize a different objective</p><p class="math-container">\[\begin{aligned}
&amp; \underset{\Theta}{\text{minimize}}
&amp; &amp; B_F\left(X \| g(\Theta) \right) + \epsilon B_F\left(\mu_0 \| g(\Theta)\right)
\end{aligned}\]</p><p>where <span>$\mu_0$</span> in any value in <span>$\mathrm{range}(g)$</span> and <span>$\epsilon$</span> is some small positive constant.</p><h1 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h1><div class="citation canonical"><ul><li><div id="EPCA">Collins, M.; Dasgupta, S. and Schapire, R. E. (2001). <em>A Generalization of Principal Components Analysis to the Exponential Family</em>. Advances in Neural Information Processing Systems <strong>14</strong>.</div></li><li><div id="GLM">McCullagh, P. and Nelder, J. A. (1989). <em>Generalized Linear Models</em>. 2 Edition, <em>Chapman &amp; Hall/CRC Monographs on Statistics and Applied                Probability</em> (Chapman &amp; Hall/CRC, Philadelphia, PA).</div></li><li><div id="PCA">Pearson, K. (1901). <em>On Lines and Planes of Closest Fit to Systems of Points in Space</em>. <a href="https://doi.org/10.1080/14786440109462720">Philosophical Magazine <strong>2</strong>, 559–572</a>.</div></li></ul></div><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>If you are not yet familiar with these concepts, I suggest exploring these resources on <a href="https://youtu.be/_-02ze7tf08?si=RzfLXbprHDQ-qSi-">gradients</a> and <a href="https://youtu.be/uQhTuRlWMxw?si=rH20Ih5A1mnyrR7f">matrix ranks</a>.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« ExpFamilyPCA.jl</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Sunday 18 August 2024 11:48">Sunday 18 August 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
