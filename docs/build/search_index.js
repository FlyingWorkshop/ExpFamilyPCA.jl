var documenterSearchIndex = {"docs":
[{"location":"constructors/poisson/#Poisson-EPCA","page":"Poisson","title":"Poisson EPCA","text":"","category":"section"},{"location":"constructors/poisson/","page":"Poisson","title":"Poisson","text":"Name PoissonEPCA\nG(theta) e^theta\ng(theta) e^theta\nmu Space[1] positive\nTheta Space real\nAppropriate Data count, probability","category":"page"},{"location":"constructors/poisson/","page":"Poisson","title":"Poisson","text":"Poisson EPCA minimizes the generalized KL divergence making it well-suited for compressing probability profiles. Poisson EPCA has also been used in reinforcement learning to solve partially observed Markov decision processes (POMDPs) with belief compression (Roy et al., 2005). ","category":"page"},{"location":"constructors/poisson/","page":"Poisson","title":"Poisson","text":"[1]: mu space refers to the space of valid regularization parameters, not to the expectation parameter space.","category":"page"},{"location":"constructors/poisson/#Documentation","page":"Poisson","title":"Documentation","text":"","category":"section"},{"location":"constructors/poisson/","page":"Poisson","title":"Poisson","text":"PoissonEPCA","category":"page"},{"location":"constructors/poisson/#ExpFamilyPCA.PoissonEPCA","page":"Poisson","title":"ExpFamilyPCA.PoissonEPCA","text":"PoissonEPCA(indim::Integer, outdim::Integer; options::Options = Options())\n\nAn EPCA model with Poisson loss.\n\nArguments\n\nindim::Integer: Dimension of the input space.\noutdim::Integer: Dimension of the latent (output) space.\noptions::Options: Optional parameters.\n\nReturns\n\nepca: An EPCA subtype for the Poisson distribution.\n\n\n\n\n\n","category":"function"},{"location":"constructors/gaussian/#Gaussian-EPCA","page":"Gaussian","title":"Gaussian EPCA","text":"","category":"section"},{"location":"constructors/gaussian/","page":"Gaussian","title":"Gaussian","text":"Name GaussianEPCA or NormalEPCA\nG(theta) theta^2  2\ng(theta) theta\nmu Space[1] real\nTheta Space real\nAppropriate Data continuous","category":"page"},{"location":"constructors/gaussian/","page":"Gaussian","title":"Gaussian","text":"The Gaussian EPCA objective is equivalent to regular PCA.","category":"page"},{"location":"constructors/gaussian/","page":"Gaussian","title":"Gaussian","text":"[1]: mu space refers to the space of valid regularization parameters, not to the expectation parameter space.","category":"page"},{"location":"constructors/gaussian/#Documentation","page":"Gaussian","title":"Documentation","text":"","category":"section"},{"location":"constructors/gaussian/","page":"Gaussian","title":"Gaussian","text":"GaussianEPCA","category":"page"},{"location":"constructors/gaussian/#ExpFamilyPCA.GaussianEPCA","page":"Gaussian","title":"ExpFamilyPCA.GaussianEPCA","text":"Alias for NormalEPCA.\n\n\n\n\n\n","category":"function"},{"location":"constructors/gaussian/","page":"Gaussian","title":"Gaussian","text":"NormalEPCA","category":"page"},{"location":"constructors/gaussian/#ExpFamilyPCA.NormalEPCA","page":"Gaussian","title":"ExpFamilyPCA.NormalEPCA","text":"GaussianEPCA(indim::Integer, outdim::Integer; options::Options = Options(V_init_value = 0))\n\nAn EPCA model with Gaussian loss.\n\nArguments\n\nindim::Integer: Dimension of the input space.\noutdim::Integer: Dimension of the latent (output) space.\noptions::Options: Optional parameters (default: V_init_value = 0).\n\nReturns\n\nepca: An EPCA subtype for the Gaussian distribution.\n\n\n\n\n\n","category":"function"},{"location":"constructors/weibull/#Weibull-EPCA","page":"Weibull","title":"Weibull EPCA","text":"","category":"section"},{"location":"constructors/weibull/","page":"Weibull","title":"Weibull","text":"Name WeibullEPCA\nG(theta) -log(-theta) - log k\ng(theta) -frac1theta\nmu Space[1] mathbbR   0 \nTheta Space `negative\nAppropriate Data nonnegative continuous","category":"page"},{"location":"constructors/weibull/","page":"Weibull","title":"Weibull","text":"WeibullEPCA omits it the known shape parameter k since it does not affect the Weibull EPCA objective.","category":"page"},{"location":"constructors/weibull/","page":"Weibull","title":"Weibull","text":"[1]: mu space refers to the space of valid regularization parameters, not to the expectation parameter space.","category":"page"},{"location":"constructors/weibull/#Documentation","page":"Weibull","title":"Documentation","text":"","category":"section"},{"location":"constructors/weibull/","page":"Weibull","title":"Weibull","text":"WeibullEPCA","category":"page"},{"location":"constructors/weibull/#ExpFamilyPCA.WeibullEPCA","page":"Weibull","title":"ExpFamilyPCA.WeibullEPCA","text":"WeibullEPCA(indim::Integer, outdim::Integer; options::Options = Options(A_init_value = -1, A_upper = -eps(), V_lower = eps()))\n\nAn EPCA model with Weibull loss.\n\nArguments\n\nindim::Integer: Dimension of the input space.\noutdim::Integer: Dimension of the latent (output) space.\noptions::Options: Optional parameters for model initialization:\nA_init_value: Initial fill value for matrix A (default: -1).\nA_upper: Upper bound for matrix A (default: -eps()).\nV_lower: Lower bound for matrix V (default: eps()).\n\nReturns\n\nepca: An EPCA subtype for the Weibull distribution.\n\n\n\n\n\n","category":"function"},{"location":"math/bregman/#Bregman-Divergences","page":"Bregman Divergences","title":"Bregman Divergences","text":"","category":"section"},{"location":"math/bregman/#Definition","page":"Bregman Divergences","title":"Definition","text":"","category":"section"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"A Bregman divergence (Bregman, 1967) denoted B_F is defined with respect to a strictly convex and differentiable function F Omega to mathbbR, by ","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"B_F(p q) = F(p) - F(q) - langle f(p) p - q rangle","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"where langle  rangle denotes an inner product and f(x) = nabla_x F(x). Intuitively, the Bregman divergence expresses the difference at p between F and its first-order Taylor expansion about q. We use Bregman divergences to measure the difference between two probability distributions. The Bregman divergence is also sometimes called the Bregman distance, but it is not a metric since it usually satisfies neither symmetry nor the triangle inequality.","category":"page"},{"location":"math/bregman/#Relationship-to-the-Exponential-Family","page":"Bregman Divergences","title":"Relationship to the Exponential Family","text":"","category":"section"},{"location":"math/bregman/#The-Exponential-Family","page":"Bregman Divergences","title":"The Exponential Family","text":"","category":"section"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"A distribution is said to be in the natural exponential family if its density can be written","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"p(x  theta) = P_0(x) exp(langle x theta rangle - G(theta) )","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"where x and theta are vectors in mathbbR^d, P_0 is a known function that does not depend on theta, and G is the log-partition function.  Intuitively, the log-partition function ensures that the p is a valid distribution, meaning it integrates to 1","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"G(theta) = log int P_0(x) exp(langle x theta rangle) dx","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"We call theta the natural parameter and mu = mathbbE_theta sim p(cdot theta)x the expectation parameter. For the exponential family (and assuming some standard regularity conditions), we have mu = nabla_theta G(theta) equiv g(theta) (McCullagh and Nelder, 1989; Azoury and Warmuth, 2001). Since G is strictly convex, we can also define the inverse g^-1(mu) equiv theta.","category":"page"},{"location":"math/bregman/#The-Legendre-Transform-and-Parameter-Duality","page":"Bregman Divergences","title":"The Legendre Transform and Parameter Duality","text":"","category":"section"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"To understand the relationship between expectation parameters and natural parameters, first recall the Legendre transform from physics. For a convex function h, the Legendre transform is ","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"h^*(tildex) equiv tildex cdot x - f(x)","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"We say that h^* is the dual (or convex conjugate) of h. Let F be the dual G","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"F(mu) equiv langle mu theta rangle - G(theta)","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"Observe that the gradient of the dual is the inverse of the gradient of the log-partition,","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"beginaligned\n\nf(mu) \nequiv nabla_mu F(mu) \n= nabla_mu Big langle mu theta rangle - G(theta)Big \n= theta + langle mu nabla_mu theta rangle - langle g(theta) nabla_mu theta rangle text(theta is a function of mu)\n= theta text(mu = g(theta)) \n= g^-1(mu)\nendaligned","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"In summary, the parameterizations are related by the Legendre transformations ","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"nabla_theta G(theta) = g(theta) = mu","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"and","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"nabla_mu F_(mu) = f(mu) = theta","category":"page"},{"location":"math/bregman/#Bregman-Divergences-as-Loss-Functions","page":"Bregman Divergences","title":"Bregman Divergences as Loss Functions","text":"","category":"section"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"The key relationship between members of the exponential family and the Bregman divergence is this: minimizing the negative log-likelihood of p(x theta) is equivalent to minimizing the Bregman divergence B_F. To see this, first recall that the negative log-likelihood for members of the exponential family is G(theta) - langle x theta rangle. ","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"langle x theta rangle - G(theta)","category":"page"},{"location":"math/bregman/#Other-Resources","page":"Bregman Divergences","title":"Other Resources","text":"","category":"section"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"Mark Reid provides an excellent and accessible introduction to Bregman divergences.","category":"page"},{"location":"constructors/continuous_bernoulli/#Continuous-Bernoulli-EPCA","page":"Continuous Bernoulli","title":"Continuous Bernoulli EPCA","text":"","category":"section"},{"location":"constructors/continuous_bernoulli/#Math","page":"Continuous Bernoulli","title":"Math","text":"","category":"section"},{"location":"constructors/continuous_bernoulli/","page":"Continuous Bernoulli","title":"Continuous Bernoulli","text":"Name ContinuousBernoulliEPCA\nG(theta) log frace^theta - 1theta\ng(theta) fractheta-1theta + frac1e^theta - 1\nmu Space[1] (0 1) setminus frac12\nTheta Space real\nAppropriate Data unit interval","category":"page"},{"location":"constructors/continuous_bernoulli/","page":"Continuous Bernoulli","title":"Continuous Bernoulli","text":"[1]: mu space refers to the space of valid regularization parameters, not to the expectation parameter space.","category":"page"},{"location":"constructors/continuous_bernoulli/#Documentation","page":"Continuous Bernoulli","title":"Documentation","text":"","category":"section"},{"location":"constructors/continuous_bernoulli/","page":"Continuous Bernoulli","title":"Continuous Bernoulli","text":"ContinuousBernoulliEPCA","category":"page"},{"location":"constructors/continuous_bernoulli/#ExpFamilyPCA.ContinuousBernoulliEPCA","page":"Continuous Bernoulli","title":"ExpFamilyPCA.ContinuousBernoulliEPCA","text":"ContinuousBernoulliEPCA(indim::Integer, outdim::Integer; options::Options = Options(μ = 0.5))\n\nAn EPCA model with continuous Bernoulli loss.\n\nArguments\n\nindim::Integer: Dimension of the input space.\noutdim::Integer: Dimension of the latent (output) space.\noptions::Options: Optional parameters (default: μ = 0.25).\n\nReturns\n\nepca: An EPCA subtype for the continuous Bernoulli distribution.\n\n\n\n\n\n","category":"function"},{"location":"math/gamma/#Gamma-EPCA","page":"Gamma EPCA and the Itakura-Saito Distance","title":"Gamma EPCA","text":"","category":"section"},{"location":"math/gamma/","page":"Gamma EPCA and the Itakura-Saito Distance","title":"Gamma EPCA and the Itakura-Saito Distance","text":"The cumulant of the gamma distribution is G(theta) = -log(-theta), so the the link function (its derivative) is g(theta) = nabla_theta G(theta) = -frac1theta. From the Legendre transform, we know that f(x) = g^-1(x) = -frac1x and ","category":"page"},{"location":"math/gamma/","page":"Gamma EPCA and the Itakura-Saito Distance","title":"Gamma EPCA and the Itakura-Saito Distance","text":"beginaligned\nF(x) \n= theta cdot x - G(theta) \n= f(x) cdot x - G(f(x)) \n= -1 - log(x)\nendaligned","category":"page"},{"location":"math/gamma/","page":"Gamma EPCA and the Itakura-Saito Distance","title":"Gamma EPCA and the Itakura-Saito Distance","text":"The Bregman divergence induced from F is","category":"page"},{"location":"math/gamma/","page":"Gamma EPCA and the Itakura-Saito Distance","title":"Gamma EPCA and the Itakura-Saito Distance","text":"beginaligned\nB_F(p q) \n= F(p) - F(q) - langle f(q) p - q rangle \n= -1 - log p + 1 + log q + Biglangle frac1q p - q Bigrangle \n= fracpq - log fracpq - 1\nendaligned","category":"page"},{"location":"math/gamma/","page":"Gamma EPCA and the Itakura-Saito Distance","title":"Gamma EPCA and the Itakura-Saito Distance","text":"so  B_F is the Itakura-Saito distance as desired. Further, the EPCA objective is","category":"page"},{"location":"math/gamma/","page":"Gamma EPCA and the Itakura-Saito Distance","title":"Gamma EPCA and the Itakura-Saito Distance","text":"beginaligned\nB_F(x g(theta)) = fracpg(theta) - log fracpg(theta) - 1 = -ptheta - log(-ptheta) - 1\nendaligned","category":"page"},{"location":"math/derivations/#Exponential-Family-Distributions","page":"EPCA Objective Derivations","title":"Exponential Family Distributions","text":"","category":"section"},{"location":"math/derivations/","page":"EPCA Objective Derivations","title":"EPCA Objective Derivations","text":"In this section, we describe several methods to induce the EPCA objective described earlier:","category":"page"},{"location":"math/derivations/","page":"EPCA Objective Derivations","title":"EPCA Objective Derivations","text":"beginaligned\n undersetThetatextminimize\n  B_Fleft(X  g(Theta) right) + epsilon B_Fleft(mu  g(Theta)right)\nendaligned","category":"page"},{"location":"math/derivations/","page":"EPCA Objective Derivations","title":"EPCA Objective Derivations","text":"where B_F is the Bregman divergence associated with the convex conjugate F of the cumulant G of the distribution, X is our data matrix, and both mu and epsilon are hyperparameters used for normalization.","category":"page"},{"location":"math/derivations/","page":"EPCA Objective Derivations","title":"EPCA Objective Derivations","text":"note: Theorem 1\nThe EPCA objective and decompression can be expressed with F and g.","category":"page"},{"location":"math/derivations/#Case-1:-F-and-g","page":"EPCA Objective Derivations","title":"Case 1: F and g","text":"","category":"section"},{"location":"math/derivations/","page":"EPCA Objective Derivations","title":"EPCA Objective Derivations","text":"Recall that the Bregman divergence is ","category":"page"},{"location":"math/derivations/","page":"EPCA Objective Derivations","title":"EPCA Objective Derivations","text":"B_F(p q) = F(p) - F(q) - langle f(p) p - q rangle","category":"page"},{"location":"math/derivations/","page":"EPCA Objective Derivations","title":"EPCA Objective Derivations","text":"For simplicity, we drop the langle rangle notation. The (unregularized) EPCA objective seeks to minimize ","category":"page"},{"location":"math/derivations/","page":"EPCA Objective Derivations","title":"EPCA Objective Derivations","text":"beginaligned\nB_F(X g(theta)) \n= F(X) - F(g(theta)) - f(g(theta)) cdot (X - g(theta)) \n= F(X) - F(g(theta)) - theta cdot (X - g(theta))\nendaligned","category":"page"},{"location":"math/derivations/","page":"EPCA Objective Derivations","title":"EPCA Objective Derivations","text":"We can drop F(X) since it is a constant, so the EPCA objective can be written","category":"page"},{"location":"math/derivations/","page":"EPCA Objective Derivations","title":"EPCA Objective Derivations","text":"F(g(theta)) - theta cdot (X - g(theta))","category":"page"},{"location":"math/derivations/","page":"EPCA Objective Derivations","title":"EPCA Objective Derivations","text":"meaning it can be fully specified in terms of F and g. ","category":"page"},{"location":"math/derivations/#F-and-f","page":"EPCA Objective Derivations","title":"F and f","text":"","category":"section"},{"location":"math/derivations/","page":"EPCA Objective Derivations","title":"EPCA Objective Derivations","text":"We can recover g using f. To do so, recall that g = f^-1 under the Legendre transform. Because the cumulant G is strictly convex, g is monotone increasing. Thus, we can evaluate g effeciently using a binary search. Explicitly, we can evaluate g at an arbitrary input a by finding the unique root of f(x) - a. Thus, we can specify the EPCA objective and decompression with F and f. ","category":"page"},{"location":"math/derivations/#F","page":"EPCA Objective Derivations","title":"F","text":"","category":"section"},{"location":"math/derivations/","page":"EPCA Objective Derivations","title":"EPCA Objective Derivations","text":"We can differentiate F to find f and thus induce the EPCA objective.","category":"page"},{"location":"math/derivations/#Case-2:-G-and-g","page":"EPCA Objective Derivations","title":"Case 2: G and g","text":"","category":"section"},{"location":"math/derivations/","page":"EPCA Objective Derivations","title":"EPCA Objective Derivations","text":"Recall that the Bregman divergence is ","category":"page"},{"location":"math/derivations/","page":"EPCA Objective Derivations","title":"EPCA Objective Derivations","text":"B_F(p q) = F(p) - F(q) - langle f(p) p - q rangle","category":"page"},{"location":"math/derivations/","page":"EPCA Objective Derivations","title":"EPCA Objective Derivations","text":"For simplicity, we drop the langle rangle notation. The (unregularized) EPCA objective seeks to minimize ","category":"page"},{"location":"math/derivations/","page":"EPCA Objective Derivations","title":"EPCA Objective Derivations","text":"beginaligned\nB_F(X g(theta)) \n= F(X) - F(g(theta)) - f(g(theta)) cdot (X - g(theta)) \n= F(X) - F(g(theta)) - theta cdot (X - g(theta))\n= F(X) - g(theta) theta + G(theta) - theta X + g(theta)theta\n= F(X) + G(theta) - theta X\nendaligned","category":"page"},{"location":"math/derivations/","page":"EPCA Objective Derivations","title":"EPCA Objective Derivations","text":"The second and third lines follow from the Legendre transform. Explicitly, the second line follows f being the inverse of g, and the third lines follows from F being the convex conjugate of G. Since F(X) is a constant, we can drop the first term and write the final EPCA objective as ","category":"page"},{"location":"math/derivations/","page":"EPCA Objective Derivations","title":"EPCA Objective Derivations","text":"G(theta) - theta X","category":"page"},{"location":"math/derivations/","page":"EPCA Objective Derivations","title":"EPCA Objective Derivations","text":"This means that we can create an EPCA object by providing G and g (g is needed for decompression since X_textrecon = g(AV)), but we can find the link function g by symbolically differentiating G, so EPCA can be entirely specified from the cumulant.","category":"page"},{"location":"math/derivations/#Case-2:-F-and-g","page":"EPCA Objective Derivations","title":"Case 2: F and g","text":"","category":"section"},{"location":"constructors/gamma/#Gamma-EPCA","page":"Gamma","title":"Gamma EPCA","text":"","category":"section"},{"location":"constructors/gamma/#Math","page":"Gamma","title":"Math","text":"","category":"section"},{"location":"constructors/gamma/","page":"Gamma","title":"Gamma","text":"Name GammaEPCA or ItakuraSaitoEPCA\nG(theta) -log(-theta)\ng(theta) -frac1theta\nmu Space[1] mathbbR setminus  0 \nTheta Space negative\nAppropriate Data positive","category":"page"},{"location":"constructors/gamma/","page":"Gamma","title":"Gamma","text":"[1]: mu space refers to the space of valid regularization parameters, not to the expectation parameter space.","category":"page"},{"location":"constructors/gamma/#Documentation","page":"Gamma","title":"Documentation","text":"","category":"section"},{"location":"constructors/gamma/","page":"Gamma","title":"Gamma","text":"GammaEPCA","category":"page"},{"location":"constructors/gamma/#ExpFamilyPCA.GammaEPCA","page":"Gamma","title":"ExpFamilyPCA.GammaEPCA","text":"GammaEPCA(indim::Integer, outdim::Integer; options::Options = Options(A_init_value = -2, A_upper = -eps(), V_lower = eps()))\n\nGamma EPCA.\n\nArguments\n\nindim::Integer: The dimension of the input space.\noutdim::Integer: The dimension of the latent (output) space.\noptions::Options: Optional configuration parameters for the EPCA model. \nA_init_value: Initial fill value for matrix A (default: -2).\nA_upper: The upper bound for the matrix A, default is -eps().\nV_lower: The lower bound for the matrix V, default is eps().\n\nReturns\n\nepca: An instance of an EPCA subtype.\n\n\n\n\n\n","category":"function"},{"location":"constructors/gamma/","page":"Gamma","title":"Gamma","text":"ItakuraSaitoEPCA","category":"page"},{"location":"constructors/gamma/#ExpFamilyPCA.ItakuraSaitoEPCA","page":"Gamma","title":"ExpFamilyPCA.ItakuraSaitoEPCA","text":"Alias for GammaEPCA.\n\n\n\n\n\n","category":"function"},{"location":"constructors/binomial/#Binomial-EPCA","page":"Binomial","title":"Binomial EPCA","text":"","category":"section"},{"location":"constructors/binomial/#Math","page":"Binomial","title":"Math","text":"","category":"section"},{"location":"constructors/binomial/","page":"Binomial","title":"Binomial","text":"Name BinomialEPCA\nG(theta) n log(1 + e^theta)\ng(theta) fracn e^theta1+e^theta\nmu Space[1] (0 n)\nTheta Space real\nAppropriate Data count\nn n geq 0 (number of trials)","category":"page"},{"location":"constructors/binomial/","page":"Binomial","title":"Binomial","text":"G is the scaled softplus function and g is the scaled logistic function.","category":"page"},{"location":"constructors/binomial/","page":"Binomial","title":"Binomial","text":"[1]: mu space refers to the space of valid regularization parameters, not to the expectation parameter space.","category":"page"},{"location":"constructors/binomial/#Documentation","page":"Binomial","title":"Documentation","text":"","category":"section"},{"location":"constructors/binomial/","page":"Binomial","title":"Binomial","text":"BinomialEPCA","category":"page"},{"location":"constructors/binomial/#ExpFamilyPCA.BinomialEPCA","page":"Binomial","title":"ExpFamilyPCA.BinomialEPCA","text":"BinomialEPCA(indim::Integer, outdim::Integer, n::Integer; options::Options = Options(μ = 0.5))\n\nAn EPCA model with binomial loss.\n\nArguments\n\nindim::Integer: Dimension of the input space.\noutdim::Integer: Dimension of the latent (output) space.\nn::Integer: A known parameter representing the number of trials (nonnegative).\noptions::Options: Optional parameters (default: μ = 0.5).\n\nReturns\n\nepca: An EPCA subtype for the binomial distribution.\n\n\n\n\n\n","category":"function"},{"location":"constructors/negative_binomial/#Negative-Binomial-EPCA","page":"Negative Binomial","title":"Negative Binomial EPCA","text":"","category":"section"},{"location":"constructors/negative_binomial/","page":"Negative Binomial","title":"Negative Binomial","text":"Name NegativeBinomialEPCA\nG(theta) -r log(1 - e^theta)\ng(theta) - fracr  e^theta1 - e^theta\nmu Space[1] positive\nTheta Space negative\nAppropriate Data count\nr r  0 (number of successes)","category":"page"},{"location":"constructors/negative_binomial/","page":"Negative Binomial","title":"Negative Binomial","text":"[1]: mu space refers to the space of valid regularization parameters, not to the expectation parameter space.","category":"page"},{"location":"constructors/negative_binomial/#Documentation","page":"Negative Binomial","title":"Documentation","text":"","category":"section"},{"location":"constructors/negative_binomial/","page":"Negative Binomial","title":"Negative Binomial","text":"NegativeBinomialEPCA","category":"page"},{"location":"constructors/negative_binomial/#ExpFamilyPCA.NegativeBinomialEPCA","page":"Negative Binomial","title":"ExpFamilyPCA.NegativeBinomialEPCA","text":"GammaEPCA(indim::Integer, outdim::Integer, r::Integer; options::Options = Options(A_init_value = -2, A_upper = -eps(), V_lower = eps()))\n\nGamma EPCA.\n\nArguments\n\nindim::Integer: The dimension of the input space.\noutdim::Integer: The dimension of the latent (output) space.\nr::Integer: A known parameter of the negative binomial distribution representing the number of successes (positive).\noptions::Options: Optional configuration parameters for the EPCA model. \nA_init_value: Initial fill value for matrix A (default: -1).\nA_upper: The upper bound for the matrix A, default is -eps().\nV_lower: The lower bound for the matrix V, default is eps().\n\nReturns\n\nepca: An instance of an EPCA subtype.\n\n\n\n\n\n","category":"function"},{"location":"constructors/pareto/#Pareto-EPCA","page":"Pareto","title":"Pareto EPCA","text":"","category":"section"},{"location":"constructors/pareto/#Math","page":"Pareto","title":"Math","text":"","category":"section"},{"location":"constructors/pareto/","page":"Pareto","title":"Pareto","text":"Name ParetoEPCA\nG(theta) -log(-1 - theta) + theta log m\ng(theta) log m - frac1theta + 1\nmu Space[1] mathbbR setminus  logm \nTheta Space negative\nAppropriate Data continuous\nm m  0 (minimum value)","category":"page"},{"location":"constructors/pareto/","page":"Pareto","title":"Pareto","text":"[1]: mu space refers to the space of valid regularization parameters, not to the expectation parameter space.","category":"page"},{"location":"constructors/pareto/#Documentation","page":"Pareto","title":"Documentation","text":"","category":"section"},{"location":"constructors/pareto/","page":"Pareto","title":"Pareto","text":"ParetoEPCA","category":"page"},{"location":"constructors/pareto/#ExpFamilyPCA.ParetoEPCA","page":"Pareto","title":"ExpFamilyPCA.ParetoEPCA","text":"ParetoEPCA(indim::Integer, outdim::Integer, m::Real; options::Options = Options(μ = 2, A_init_value = 2, A_lower = 1 / indim, V_init_value = -2, V_upper = -1))\n\nAn EPCA model with Pareto loss.\n\nArguments\n\nindim::Integer: Dimension of the input space.\noutdim::Integer: Dimension of the latent (output) space.\nm::Real: A known parameter of the Pareto distribution representing the minimum value in the support.\noptions::Options: Optional parameters for model initialization:\nμ: Default value 2.\nA_init_value: Initial value for matrix A (default: 2).\nA_lower: Lower bound for matrix A (default: 1 / indim).\nV_init_value: Initial value for matrix V (default: -2).\nV_upper: Upper bound for matrix V (default: -1).\n\nReturns\n\nepca: An EPCA subtype for the Pareto distribution.\n\n\n\n\n\n","category":"function"},{"location":"api/#API-Documentation","page":"API Documentation","title":"API Documentation","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"CurrentModule = ExpFamilyPCA","category":"page"},{"location":"api/#Contents","page":"API Documentation","title":"Contents","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/#Index","page":"API Documentation","title":"Index","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/#Functions","page":"API Documentation","title":"Functions","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"EPCA\nfit!\ncompress\ndecompress","category":"page"},{"location":"api/#ExpFamilyPCA.EPCA","page":"API Documentation","title":"ExpFamilyPCA.EPCA","text":"NOTE: μ must be in the range of g, so g⁻¹(μ) is finite. It is up to the user to enforce this.\n\n\n\n\n\n","category":"type"},{"location":"api/#ExpFamilyPCA.fit!","page":"API Documentation","title":"ExpFamilyPCA.fit!","text":"fit!(epca::EPCA, X::AbstractMatrix{T}; maxiter::Integer = 100, verbose::Bool = false, steps_per_print::Integer = 10) where T <: Real\n\nFits the Exponential Family Principal Component Analysis (EPCA) model to the given dataset X.\n\nThe fit! function optimizes the parameters of an EPCA model to minimize a loss function specific to the chosen exponential family distribution of the data. This optimization process adjusts the model's internal parameters to achieve a lower-dimensional representation of the data that captures the most significant variance while adhering to the constraints of the distribution family.\n\nArguments\n\nepca::EPCA: An instance of the EPCA model. This object specifies the structure of the model and the assumptions about the distribution of the input data. It must be an instance of a subtype of the abstract type EPCA.\nX::AbstractMatrix{T}: The input data matrix where each row represents an observation and each column represents a feature or variable. T is a subtype of Real, indicating that the data should consist of real numbers (e.g., Float64, Float32).\n\nKeyword Arguments\n\nmaxiter::Integer = 100: The maximum number of iterations to perform during the optimization process. Each iteration updates the model parameters to reduce the loss function. Defaults to 100.\nverbose::Bool = false: A flag indicating whether to print progress information during the optimization process. If set to true, the function prints the loss value and iteration number at specified intervals (steps_per_print). Defaults to false.\nsteps_per_print::Integer = 10: The number of iterations between printed progress updates when verbose is set to true. For example, if steps_per_print is 10, progress will be printed every 10 iterations. Defaults to 10.\n\nReturns\n\nA::AbstractMatrix{T}: The optimized auxiliary parameter matrix A that minimizes the loss function for the model. This matrix represents the lower-dimensional representation of the input data X in the reduced space defined by the EPCA model. The matrix A can be used for further data compression and reconstruction tasks.\n\nExamples\n\n# Import the module\nusing ExpFamilyPCA\n\n# Define the EPCA model parameters\nindim = 100  # Input dimension\noutdim = 10  # Reduced output dimension\nG = x -> log(1 + exp(x))  # Log-partition function for Bernoulli data\ng = x -> 1 / (1 + exp(-x))  # Sigmoid link function\n\n# Create an EPCA model instance\nepca_model = EPCA(indim, outdim, G, g, Val((:G, :g)))\n\n# Generate some random input data\nX = rand(Float64, 1000, indim)  # 1000 observations, each with 100 features\n\n# Fit the model to the data\nA = fit!(epca_model, X; maxiter=200, verbose=true, steps_per_print=20)\n\n# The resulting matrix A is the lower-dimensional representation of X\n\nNotes\n\nIt is recommended to preprocess the input data X (e.g., normalization or scaling) to ensure better convergence and numerical stability during optimization.\n\n\n\n\n\n","category":"function"},{"location":"api/#ExpFamilyPCA.compress","page":"API Documentation","title":"ExpFamilyPCA.compress","text":"compress(epca::EPCA, X::AbstractMatrix{T}; maxiter::Integer = 100, verbose::Bool = false, steps_per_print::Integer = 10) where T <: Real\n\nCompresses the input data X using a fitted Exponential Family Principal Component Analysis (EPCA) model.\n\nThe compress function projects the input data X into a lower-dimensional space defined by the fitted EPCA model. This compression process reduces the dimensionality of the data while preserving its most significant features, as dictated by the model's parameters and the underlying exponential family distribution.\n\nArguments\n\nepca::EPCA: A pre-fitted instance of the EPCA model. This object should have its parameters already optimized by a prior call to fit!, specifying the structure of the model and the distributional assumptions of the data.\nX::AbstractMatrix{T}: The input data matrix to be compressed, where each row represents an observation and each column represents a feature. T must be a subtype of Real, indicating the data consists of real numbers (e.g., Float64, Float32).\n\nKeyword Arguments\n\nmaxiter::Integer = 100: The maximum number of iterations for the optimization process to compress the data. Each iteration updates the auxiliary parameter matrix A to best represent the data in the reduced space. Defaults to 100.\nverbose::Bool = false: A flag indicating whether to print progress information during the compression process. If set to true, the function prints the loss value and iteration number at specified intervals (steps_per_print). Defaults to false.\nsteps_per_print::Integer = 10: The number of iterations between printed progress updates when verbose is set to true. For example, if steps_per_print is 10, progress will be printed every 10 iterations. Defaults to 10.\n\nReturns\n\nA::AbstractMatrix{T}: The compressed representation of the input data X. This matrix A represents the input data in the lower-dimensional space defined by the EPCA model, retaining the essential structure and patterns of the data with reduced dimensionality.\n\nExamples\n\n# Import the module\nusing ExpFamilyPCA\n\n# Define the EPCA model parameters\nindim = 100  # Input dimension\noutdim = 10  # Reduced output dimension\nG = x -> log(1 + exp(x))  # Log-partition function for Bernoulli data\ng = x -> 1 / (1 + exp(-x))  # Sigmoid link function\n\n# Create and fit an EPCA model instance\nepca_model = EPCA(indim, outdim, G, g, Val((:G, :g)))\nX = rand(Float64, 1000, indim)  # Generate some random input data\nfit!(epca_model, X; maxiter=200, verbose=true)\n\n# Compress the data using the fitted model\nA_compressed = compress(epca_model, X; maxiter=100, verbose=true, steps_per_print=20)\n\n# The resulting matrix A_compressed is the lower-dimensional representation of X\n\n\n\n\n\n","category":"function"},{"location":"api/#ExpFamilyPCA.decompress","page":"API Documentation","title":"ExpFamilyPCA.decompress","text":"decompress(epca::EPCA, A::AbstractMatrix{T}) where T <: Real\n\nReconstructs the original data from its compressed form using a fitted Exponential Family Principal Component Analysis (EPCA) model.\n\nThe decompress function takes a compressed representation A of the data and reconstructs an approximation of the original data in its full-dimensional space. This reconstruction is based on the learned parameters of the EPCA model, which captures the underlying patterns and structure of the original dataset.\n\nArguments\n\nepca::EPCA: A fitted instance of the EPCA model. This object should have been optimized with a prior call to fit! and should reflect the structure and distributional assumptions of the original data.\nA::AbstractMatrix{T}: The compressed data matrix, where each row represents a lower-dimensional observation and each column represents a reduced feature. T must be a subtype of Real, indicating the data consists of real numbers (e.g., Float64, Float32).\n\nReturns\n\nX̂::AbstractMatrix{T}: The reconstructed data matrix in its original dimensionality. This matrix X̂ is an approximation of the original input data X, reconstructed using the parameters of the EPCA model.\n\nExamples\n\n# Import the module\nusing ExpFamilyPCA\n\n# Define the EPCA model parameters\nindim = 100  # Input dimension\noutdim = 10  # Reduced output dimension\nG = x -> log(1 + exp(x))  # Log-partition function for Bernoulli data\ng = x -> 1 / (1 + exp(-x))  # Sigmoid link function\n\n# Create and fit an EPCA model instance\nepca_model = EPCA(indim, outdim, G, g, Val((:G, :g)))\nX = rand(Float64, 1000, indim)  # Generate some random input data\nfit!(epca_model, X; maxiter=200, verbose=true)\n\n# Compress the data using the fitted model\nA_compressed = compress(epca_model, X; maxiter=100, verbose=true, steps_per_print=20)\n\n# Decompress the data to approximate the original data\nX_reconstructed = decompress(epca_model, A_compressed)\n\n# The resulting matrix X_reconstructed is an approximation of the original data X\n\nNotes\n\nThe decompression process uses the link function g` defined in the EPCA model to transform the natural parameters back into the mean parameters of the distribution.\nThis function assumes that the input A` is a valid compressed representation obtained via the compress function with the same EPCA model.\n\n\n\n\n\n","category":"function"},{"location":"api/#Miscellaneous","page":"API Documentation","title":"Miscellaneous","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"EPCACompressor","category":"page"},{"location":"api/#ExpFamilyPCA.EPCACompressor","page":"API Documentation","title":"ExpFamilyPCA.EPCACompressor","text":"Compressor from [CompressedBeliefMDPs.jl](https://juliapomdp.github.io/CompressedBeliefMDPs.jl/stable/).\n\n\n\n\n\n","category":"type"},{"location":"constructors/bernoulli/#Bernoulli-EPCA","page":"Bernoulli","title":"Bernoulli EPCA","text":"","category":"section"},{"location":"constructors/bernoulli/#Math","page":"Bernoulli","title":"Math","text":"","category":"section"},{"location":"constructors/bernoulli/","page":"Bernoulli","title":"Bernoulli","text":"Name BernoulliEPCA\nG(theta) log(1 + e^theta)\ng(theta) frace^theta1+e^theta\nmu Space[1] (0 1)\nTheta Space real\nAppropriate Data binary","category":"page"},{"location":"constructors/bernoulli/","page":"Bernoulli","title":"Bernoulli","text":"G is the softplus function and g is the logistic function.","category":"page"},{"location":"constructors/bernoulli/","page":"Bernoulli","title":"Bernoulli","text":"[1]: mu space refers to the space of valid regularization parameters, not to the expectation parameter space.","category":"page"},{"location":"constructors/bernoulli/#Documentation","page":"Bernoulli","title":"Documentation","text":"","category":"section"},{"location":"constructors/bernoulli/","page":"Bernoulli","title":"Bernoulli","text":"BernoulliEPCA","category":"page"},{"location":"constructors/bernoulli/#ExpFamilyPCA.BernoulliEPCA","page":"Bernoulli","title":"ExpFamilyPCA.BernoulliEPCA","text":"BernoulliEPCA(indim::Integer, outdim::Integer; options = Options(μ = 0.5))\n\nAn EPCA model with Bernoulli loss.\n\nArguments\n\nindim::Integer: Dimension of the input space.\noutdim::Integer: Dimension of the latent (output) space.\noptions::Options: Optional parameters (default: μ = 0.5).\n\nReturns\n\nepca: An EPCA subtype for the Bernoulli distribution.\n\n\n\n\n\n","category":"function"},{"location":"#ExpFamilyPCA.jl-Documentation","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl Documentation","text":"","category":"section"},{"location":"","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl","text":"ExpFamilyPCA.jl is a Julia package for performing exponential principal component analysis (EPCA). ExpFamilyPCA.jl supports custom objectives and includes fast implementations for several common distributions.","category":"page"},{"location":"#Installation","page":"ExpFamilyPCA.jl","title":"Installation","text":"","category":"section"},{"location":"","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl","text":"To install the package, use the Julia package manager. In the Julia REPL, type:","category":"page"},{"location":"","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl","text":"using Pkg; Pkg.add(\"ExpFamilyPCA\")","category":"page"},{"location":"#Quickstart","page":"ExpFamilyPCA.jl","title":"Quickstart","text":"","category":"section"},{"location":"","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl","text":"using ExpFamilyPCA\n\nindim = 5\nX = rand(1:100, (10, indim))  # data matrix to compress\noutdim = 3  # target compression dimension\n\npoisson_epca = PoissonEPCA(indim, outdim)\n\nX_compressed = fit!(poisson_epca, X; maxiter=200, verbose=true)\n\nY = rand(1:100, (3, indim))  # test data\nY_compressed = compress(poisson_epca, Y; maxiter=200, verbose=true)\n\nX_reconstructed = decompress(poisson_epca, X_compressed)\nY_reconstructed = decompress(poisson_epca, Y_compressed)","category":"page"},{"location":"#Supported-Distributions","page":"ExpFamilyPCA.jl","title":"Supported Distributions","text":"","category":"section"},{"location":"","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl","text":"Distribution ExpFamilyPCA.jl Objective Link Function g(theta)\nBernoulli BernoulliEPCA log(1 + e^theta - 2xtheta) frace^theta1 + e^theta\nBinomial BinomialEPCA n log(1 + e^theta) - xtheta fracne^theta1 + e^theta\nContinuous Bernoulli ContinuousBernoulliEPCA logleft(frace^theta - 1thetaright) - xtheta fractheta - 1theta + frac1e^theta - 1\nGamma¹ GammaEPCA or ItakuraSaitoEPCA -log(-theta) - xtheta -frac1theta\nGaussian² GaussianEPCA or NormalEPCA frac12(x - theta)^2 theta\nNegative Binomial NegativeBinomialEPCA -r log(1 - e^theta) - xtheta frac-re^thetae^theta - 1\nPareto ParetoEPCA -log(-1 - theta) + theta log m - xtheta log m - frac1theta + 1\nPoisson³ PoissonEPCA e^theta - xtheta e^theta\nWeibull WeibullEPCA -log(-theta) - xtheta -frac1theta","category":"page"},{"location":"","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl","text":"¹: For the Gamma distribution, the link function is typically based on the inverse relationship.  ","category":"page"},{"location":"","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl","text":"²: For Gaussian, also known as Normal distribution, the link function is the identity. ","category":"page"},{"location":"","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl","text":"³: The Poisson distribution link function is exponential.","category":"page"},{"location":"math/epca/#Math-Details","page":"Exponential Family PCA","title":"Math Details","text":"","category":"section"},{"location":"math/epca/","page":"Exponential Family PCA","title":"Exponential Family PCA","text":"This page serves as an introduction to exponential family principal component analysis (EPCA) (Collins et al., 2001). I aim to keep the material accessible to those with knowledge of college-level multivariable calculus and linear algebra (e.g., gradients, matrix rank).[1] For clarity and accessibility, I will attempt to write out non-trivial steps in (sometimes exhaustive) detail. We invite readers seeking a more concise, formal presentation of EPCA to explore the original paper or my more formal treatment of Bregman divergences.","category":"page"},{"location":"math/epca/","page":"Exponential Family PCA","title":"Exponential Family PCA","text":"[1]: If you are not yet familiar with these concepts, I suggest exploring these resources on gradients and matrix ranks.","category":"page"},{"location":"math/epca/#Principal-Component-Analysis-(PCA)","page":"Exponential Family PCA","title":"Principal Component Analysis (PCA)","text":"","category":"section"},{"location":"math/epca/","page":"Exponential Family PCA","title":"Exponential Family PCA","text":"Principal component analysis (Pearson, 1901) is the most fundamental dimension reduction algorithm. It is important in data science and machine learning because it helps represent the key features of complex, high-dimensional data in an efficient, low-dimensional projection. ","category":"page"},{"location":"math/epca/","page":"Exponential Family PCA","title":"Exponential Family PCA","text":"PCA (Pearson, 1901) is a powerful dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional subspace while retaining as much variability as possible. It accomplishes this by identifying the directions of maximum variance in the data, known as principal components, and projecting the data onto these new orthogonal axes. PCA finds extensive applications in various domains, including data visualization, noise reduction, feature extraction, and exploratory data analysis.","category":"page"},{"location":"math/epca/","page":"Exponential Family PCA","title":"Exponential Family PCA","text":"PCA has many derivations and interpretations, but I only focus on two here. The first is as an intuitive geometric model; the second is as an equivalent Gaussian denoising procedure.","category":"page"},{"location":"math/epca/#The-Geometric-View","page":"Exponential Family PCA","title":"The Geometric View","text":"","category":"section"},{"location":"math/epca/","page":"Exponential Family PCA","title":"Exponential Family PCA","text":"The first interpretation of PCA is, in my opinion, the most intuitive. Given a high-dimensional dataset X in mathbbR^n times d, how can we find the nearest (w.r.t. the squared Euclidean distance) low-dimensional projection Theta? More formally, we frame this as a low-rank matrix approximation problem","category":"page"},{"location":"math/epca/","page":"Exponential Family PCA","title":"Exponential Family PCA","text":"beginaligned\n undersetThetatextminimize\n  X - Theta_F \n textsubject to\n  mathrmrankleft(Thetaright) leq ell\nendaligned","category":"page"},{"location":"math/epca/","page":"Exponential Family PCA","title":"Exponential Family PCA","text":"where  cdot _F is the Frobenius norm and ell is the target dimension. ","category":"page"},{"location":"math/epca/","page":"Exponential Family PCA","title":"Exponential Family PCA","text":"(Image: ) Figure from Parth Dholakiya.","category":"page"},{"location":"math/epca/#The-Probabilistic-View","page":"Exponential Family PCA","title":"The Probabilistic View","text":"","category":"section"},{"location":"math/epca/","page":"Exponential Family PCA","title":"Exponential Family PCA","text":"We will now show that the geometric PCA objective is equivalent to the negative log-likelihood under the unit Gaussian model with a mean at theta. First, recall that the likelihood of a unit Gaussian with mean theta is","category":"page"},{"location":"math/epca/","page":"Exponential Family PCA","title":"Exponential Family PCA","text":"L(theta) = frac1sqrt2pie^-frac(x - theta)^22","category":"page"},{"location":"math/epca/","page":"Exponential Family PCA","title":"Exponential Family PCA","text":"so the log-likelihood is","category":"page"},{"location":"math/epca/","page":"Exponential Family PCA","title":"Exponential Family PCA","text":"beginaligned\nLL(theta) = logbiggfrac1sqrt2pie^-frac(x - theta)^22bigg = -log sqrt2pi - frac(x-theta)^22\nendaligned","category":"page"},{"location":"math/epca/","page":"Exponential Family PCA","title":"Exponential Family PCA","text":"Thus, the negative log-likelihood is equivalent (up to a constant) to minimizing the Frobenius norm in the PCA low-rank matrix formulation. Intuitively, this means that the geometric task of finding the nearest low-dimensional projection is the same as the probabilistic task of recovering some low-dimensional latent structure from a high-dimensional observation with Gaussian noise.","category":"page"},{"location":"math/epca/","page":"Exponential Family PCA","title":"Exponential Family PCA","text":"(Image: )","category":"page"},{"location":"math/epca/#Exponential-Family-PCA-(EPCA)","page":"Exponential Family PCA","title":"Exponential Family PCA (EPCA)","text":"","category":"section"},{"location":"math/epca/","page":"Exponential Family PCA","title":"Exponential Family PCA","text":"EPCA (Collins et al., 2001) is an extension of PCA analogous to how generalized linear models (McCullagh and Nelder, 1989) extend linear regression. In particular, EPCA can accommodate noise drawn from any exponential family distribution. I provide a more detailed discussion of Bregman divergences and their deep relation to the exponential family here. At a high level, EPCA replaces the geometric PCA objective with a general probabilistic objective that minimizes the generalized Bregman divergence rather than the Frobenius norm (which is a special case). In particular, the EPCA objective for some strictly convex, continuously differentiable function G(theta) (often the log-partition of a parametric exponential family distribution) is ","category":"page"},{"location":"math/epca/","page":"Exponential Family PCA","title":"Exponential Family PCA","text":"beginaligned\n undersetThetatextminimize\n  B_F(X  g(Theta)) + epsilon B_F(mu  g(Theta)) \n textsubject to\n  mathrmrankleft(Thetaright) leq ell\nendaligned","category":"page"},{"location":"math/epca/","page":"Exponential Family PCA","title":"Exponential Family PCA","text":"where g is the link function (the derivative of the log-partition g(theta) = nabla_theta G(theta)), F is the convex conjugate of the log-partition, and both epsilon  0 and mu in textRange(g) are both hyperparameters used to regularize the objective (i.e., ensure real stationary points).","category":"page"},{"location":"math/epca/#References","page":"Exponential Family PCA","title":"References","text":"","category":"section"},{"location":"math/epca/","page":"Exponential Family PCA","title":"Exponential Family PCA","text":"Azoury, K. S. and Warmuth, M. K. (2001). Relative Loss Bounds for On-Line Density Estimation with the Exponential Family of Distributions. Machine learning 43, 211–246.\n\n\n\nBregman, L. (1967). The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming. USSR Computational Mathematics and Mathematical Physics 7, 200–217.\n\n\n\nCollins, M.; Dasgupta, S. and Schapire, R. E. (2001). A Generalization of Principal Components Analysis to the Exponential Family. Advances in Neural Information Processing Systems 14.\n\n\n\nMcCullagh, P. and Nelder, J. A. (1989). Generalized Linear Models. 2 Edition, Chapman & Hall/CRC Monographs on Statistics and Applied                Probability (Chapman & Hall/CRC, Philadelphia, PA).\n\n\n\nPearson, K. (1901). On Lines and Planes of Closest Fit to Systems of Points in Space. Philosophical Magazine 2, 559–572.\n\n\n\nRoy, N.; Gordon, G. and Thrun, S. (2005). Finding Approximate POMDP solutions Through Belief Compression. Journal of Artificial Intelligence Research 23, 1–40.\n\n\n\n","category":"page"}]
}
