var documenterSearchIndex = {"docs":
[{"location":"api/#API-Documentation","page":"API Documentation","title":"API Documentation","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"CurrentModule = ExpFamilyPCA","category":"page"},{"location":"api/#Contents","page":"API Documentation","title":"Contents","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/#Index","page":"API Documentation","title":"Index","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/#Functions","page":"API Documentation","title":"Functions","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"EPCA\nfit!\ncompress\ndecompress","category":"page"},{"location":"api/#ExpFamilyPCA.EPCA","page":"API Documentation","title":"ExpFamilyPCA.EPCA","text":"EPCA\n\nA flexible type representing an Exponential Family Principal Component Analysis (EPCA) model. There are many ways equivalent ways specify an EPCA model, so we provide several construction signatures. All constructions are theoretically equivalent, though some constructors are more stable and performant.\n\nThe EPCA method (and abstract type) wraps several unexported subtypes EPCA1, EPCA2 and EPCA3. Their fields differ slightly, but they share a common interface which is documented below.\n\nFields\n\nV::AbstractMatrix{<:Real}: Internal matrix used by the model.\ng::Function: The link function applied to natural parameters during the fitting process.\nμ::Real: A hyperparameter representing a positive number, used in regularization.\nϵ::Real: A hyperparameter representing a nonnegative number, used in regularization.\n\nConstructors\n\nEPCA 1\n\nEPCA(indim::Integer, outdim::Integer, F::Function, g::Function, ::Val{(:F, :g)}; μ=1, ϵ=eps())\nEPCA(indim::Integer, outdim::Integer, F::Function, f::Function, ::Val{(:F, :f)}; μ=1, ϵ=eps(), low=-1e10, high=1e10, tol=1e-10, maxiter=1e6)\nEPCA(indim::Integer, outdim::Integer, F::Function, ::Val{(:F)}; μ=1, ϵ=eps(), metaprogramming=true, low=-1e10, high=1e10, tol=1e-10, maxiter=1e6)\nEPCA(indim::Integer, outdim::Integer, F::Function, G::Function, ::Val{(:F, :G)}; μ=1, ϵ=eps(), metaprogramming=true)\n\nEPCA 2\n\nEPCA(indim::Integer, outdim::Integer, G::Function, g::Function, ::Val{(:G, :g)}; tol=eps(), μ=1, ϵ=eps())\nEPCA(indim::Integer, outdim::Integer, G::Function, ::Val{(:G)}; tol=eps(), μ=1, ϵ=eps(), metaprogramming=true)\n\nEPCA 3\n\nEPCA(indim::Integer, outdim::Integer, Bregman::Union{Function, PreMetric}, g::Function, ::Val{(:Bregman, :g)}; μ=1, ϵ=eps())\nEPCA(indim::Integer, outdim::Integer, Bregman::Function, G::Function, ::Val{(:Bregman, :G)}; μ=1, ϵ=eps(), metaprogramming=true)\n\nNotes\n\nThe EPCA type enforces constraints such as indim >= outdim to ensure the input dimension is not smaller than the output dimension.\nDepending on the constructor used, different forms of the loss function and internal transformations will be employed during model fitting.\n\n\n\n\n\n","category":"type"},{"location":"api/#ExpFamilyPCA.fit!","page":"API Documentation","title":"ExpFamilyPCA.fit!","text":"fit!(epca::EPCA, X::AbstractMatrix{T}; maxiter::Integer=10, verbose::Bool=false, steps_per_print::Integer=10, A_init::Union{Nothing, AbstractMatrix{T}}=nothing, autodiff::Bool=false) where T <: Real\n\nFits the EPCA model to the data matrix X.\n\nArguments\n\nepca::EPCA: The EPCA model to be fitted.\nX::AbstractMatrix{T}: The data matrix on which the model is to be fitted, where T is a subtype of Real.\nmaxiter::Integer=10: The maximum number of iterations for the fitting process (default is 10).\nverbose::Bool=false: If true, detailed progress is printed during the fitting process (default is false).\nsteps_per_print::Integer=10: The number of steps between each progress printout if verbose is true (default is every 10 steps).\nA_init::Union{Nothing, AbstractMatrix{T}}=nothing: An optional initial value for the matrix A. If not provided, the matrix will be initialized automatically.\nautodiff::Bool=false: If true, automatic differentiation is used during the fitting process (default is false).\n\nReturns\n\nA::AbstractMatrix{T}: The matrix A resulting from the fitting process.\n\nThe function updates the V parameter of the EPCA model and returns the matrix A as the result of the fitting process.\n\n\n\n\n\n","category":"function"},{"location":"api/#ExpFamilyPCA.compress","page":"API Documentation","title":"ExpFamilyPCA.compress","text":"compress(epca::EPCA, X::AbstractMatrix{T}; maxiter::Integer=10, verbose::Bool=false, steps_per_print::Integer=10, A_init::Union{Nothing, AbstractMatrix{T}}=nothing, autodiff::Bool=false) where T <: Real\n\nCompresses the data matrix X using the EPCA model.\n\nArguments\n\nepca::EPCA: The EPCA model used for compressing the data.\nX::AbstractMatrix{T}: The data matrix to be compressed, where T is a subtype of Real.\nmaxiter::Integer=10: The maximum number of iterations for the compression process (default is 10).\nverbose::Bool=false: If true, detailed progress is printed during the compression process (default is false).\nsteps_per_print::Integer=10: The number of steps between each progress printout if verbose is true (default is every 10 steps).\nA_init::Union{Nothing, AbstractMatrix{T}}=nothing: An optional initial value for the matrix A. If not provided, the matrix will be initialized automatically.\nautodiff::Bool=false: If true, automatic differentiation is used during the compression process (default is false).\n\nReturns\n\nA::AbstractMatrix{T}: The matrix A representing the compressed version of the data matrix X.\n\nThe function performs compression based on the EPCA model and returns the compressed matrix A.\n\n\n\n\n\n","category":"function"},{"location":"api/#ExpFamilyPCA.decompress","page":"API Documentation","title":"ExpFamilyPCA.decompress","text":"decompress(epca::EPCA, A::AbstractMatrix{T}) where T <: Real\n\nDecompresses the matrix A using the EPCA model to reconstruct the original data matrix.\n\nArguments\n\nepca::EPCA: The EPCA model used for decompressing the data.\nA::AbstractMatrix{T}: The compressed matrix to be decompressed, where T is a subtype of Real.\n\nReturns\n\nX̂::AbstractMatrix{T}: The reconstructed data matrix, which approximates the original data matrix before compression.\n\nThe function applies the inverse transformation defined by the EPCA model to decompress A and recover an approximation of the original data matrix.\n\n\n\n\n\n","category":"function"},{"location":"math/#Math-Details","page":"Math","title":"Math Details","text":"","category":"section"},{"location":"math/","page":"Math","title":"Math","text":"The goal of this page is to introduce and motivate exponential family principal component analysis (EPCA) (Collins et al., 2001). This guide is accessible to anyone with knowledge of basic multivariable calculus and linear algebra (e.g., gradients, matrix rank).[1] To ensure that readers can follow the math, we will write out every step and claim in exhaustive detail. We invite readers seeking a more concise, formal presentation of EPCA to explore the original paper.","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"[1]: If you are not yet familiar with these concepts, I suggest exploring these resources on gradients and matrix ranks.","category":"page"},{"location":"math/#Principal-Component-Analysis-(PCA)","page":"Math","title":"Principal Component Analysis (PCA)","text":"","category":"section"},{"location":"math/","page":"Math","title":"Math","text":"Principal component analysis (Pearson, 1901) is an extremely popular dimensionality reduction technique. It has been invented by several researchers throughout its history and appears across many fields. There are many interpretations and derivations of PCA, but we will only discuss two here. The first is PCA as a low-rank matrix approximation problem; the second is as a Gaussian denoising problem. ","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"PCA (Pearson, 1901) is a powerful dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional subspace while retaining as much variability as possible. It accomplishes this by identifying the directions of maximum variance in the data, known as principal components, and projecting the data onto these new orthogonal axes. PCA finds extensive applications in various domains, including data visualization, noise reduction, feature extraction, and exploratory data analysis.","category":"page"},{"location":"math/#Low-Rank-Matrix-Approximation","page":"Math","title":"Low-Rank Matrix Approximation","text":"","category":"section"},{"location":"math/","page":"Math","title":"Math","text":"PCA can be formulated as an low-rank matrix approximation problem. For a data matrix X, we want to find low-dimensional approximation Theta that minimizes the the sum of the squared Euclidean distances. Formally, we write","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"beginaligned\n undersetThetatextminimize\n  X - Theta_F \n textsubject to\n  mathrmrankleft(Thetaright) leq ell\nendaligned","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"where  cdot _F is the Frobenius norm. Observe that the objective is equivalent to maximizing the log likelihood of a Gaussian model. Consequently, PCA can be viewed as a denoising procedure that recovers the true low-dimensional signal Theta from a normally noised high-dimensional measurement X. ","category":"page"},{"location":"math/#Gaussian-Denoising","page":"Math","title":"Gaussian Denoising","text":"","category":"section"},{"location":"math/","page":"Math","title":"Math","text":"TODO: include image from presentation","category":"page"},{"location":"math/#Exponential-Family-PCA-(EPCA)","page":"Math","title":"Exponential Family PCA (EPCA)","text":"","category":"section"},{"location":"math/","page":"Math","title":"Math","text":"EPCA is an extension of PCA analogous to how generalized linear models (McCullagh and Nelder, 1989) extend linear regression. In particular, EPCA can denoise from any exponential family. Collins et al. (2001) showed that maximizing the log likelihood of any exponential family is directly related to minimizing the Bregman divergence","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"beginaligned \nB_F(p  q) equiv F(p) - F(q) - f(q)(p - q) \nendaligned","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"where ","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"beginaligned\n    f(mu) equiv nabla_mu F(mu) \n    F(mu) equiv theta cdot g(theta) - G(theta)\nendaligned","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"and g is the link function, g(theta) = nabla_theta G(theta), and mu = g(theta). In other words, F is the convex dual of cumulant \\citep{azoury2001relative}. We can now express the general formulation of the EPCA problem. For any differentiable convex function G, the EPCA problem is","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"beginaligned\n undersetThetatextminimize\n  B_Fleft(X  g(Theta) right)\nendaligned","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"where g is applied elementwise across Theta and B_F is the generalized Bregman divergence. Unfortunately, the optimal convergence constraints of the general problem remain unsolved. As such, in practice we minimize a different objective","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"beginaligned\n undersetThetatextminimize\n  B_Fleft(X  g(Theta) right) + epsilon B_Fleft(mu_0  g(Theta)right)\nendaligned","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"where mu_0 in any value in mathrmrange(g) and epsilon is some small positive constant.","category":"page"},{"location":"math/#References","page":"Math","title":"References","text":"","category":"section"},{"location":"math/","page":"Math","title":"Math","text":"Collins, M.; Dasgupta, S. and Schapire, R. E. (2001). A Generalization of Principal Components Analysis to the Exponential Family. Advances in Neural Information Processing Systems 14.\n\n\n\nMcCullagh, P. and Nelder, J. A. (1989). Generalized Linear Models. 2 Edition, Chapman & Hall/CRC Monographs on Statistics and Applied                Probability (Chapman & Hall/CRC, Philadelphia, PA).\n\n\n\nPearson, K. (1901). On Lines and Planes of Closest Fit to Systems of Points in Space. Philosophical Magazine 2, 559–572.\n\n\n\n","category":"page"},{"location":"#ExpFamilyPCA.jl-Documentation","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl Documentation","text":"","category":"section"},{"location":"","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl","text":"This is some text.","category":"page"}]
}
