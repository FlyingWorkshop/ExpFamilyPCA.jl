var documenterSearchIndex = {"docs":
[{"location":"math/#Math-Details","page":"Math","title":"Math Details","text":"","category":"section"},{"location":"math/","page":"Math","title":"Math","text":"The goal of this page is to introduce and motivate exponential family principal component analysis (EPCA) (Collins et al., 2001). This guide is accessible to anyone with knowledge of basic multivariable calculus and linear algebra (e.g., gradients, matrix rank).[1] To ensure that readers can follow the math, we will write out every step and claim in exhaustive detail. We invite readers seeking a more concise, formal presentation of EPCA to explore the original paper.","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"[1]: If you are not yet familiar with these concepts, I suggest exploring these resources on gradients and matrix ranks.","category":"page"},{"location":"math/#Principal-Component-Analysis-(PCA)","page":"Math","title":"Principal Component Analysis (PCA)","text":"","category":"section"},{"location":"math/","page":"Math","title":"Math","text":"Principal component analysis (Pearson, 1901) is an extremely popular dimensionality reduction technique. It has been invented by several researchers throughout its history and appears across many fields. There are many interpretations and derivations of PCA, but we will only discuss two here. The first is PCA as a low-rank matrix approximation problem; the second is as a Gaussian denoising problem. ","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"PCA (Pearson, 1901) is a powerful dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional subspace while retaining as much variability as possible. It accomplishes this by identifying the directions of maximum variance in the data, known as principal components, and projecting the data onto these new orthogonal axes. PCA finds extensive applications in various domains, including data visualization, noise reduction, feature extraction, and exploratory data analysis.","category":"page"},{"location":"math/#Low-Rank-Matrix-Approximation","page":"Math","title":"Low-Rank Matrix Approximation","text":"","category":"section"},{"location":"math/","page":"Math","title":"Math","text":"PCA can be formulated as an low-rank matrix approximation problem. For a data matrix X, we want to find low-dimensional approximation Theta that minimizes the the sum of the squared Euclidean distances. Formally, we write","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"beginaligned\n undersetThetatextminimize\n  X - Theta_F \n textsubject to\n  mathrmrankleft(Thetaright) leq ell\nendaligned","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"where  cdot _F is the Frobenius norm. Observe that the objective is equivalent to maximizing the log likelihood of a Gaussian model. Consequently, PCA can be viewed as a denoising procedure that recovers the true low-dimensional signal Theta from a normally noised high-dimensional measurement X. ","category":"page"},{"location":"math/#Gaussian-Denoising","page":"Math","title":"Gaussian Denoising","text":"","category":"section"},{"location":"math/","page":"Math","title":"Math","text":"TODO: include image from presentation","category":"page"},{"location":"math/#Exponential-Family-PCA-(EPCA)","page":"Math","title":"Exponential Family PCA (EPCA)","text":"","category":"section"},{"location":"math/","page":"Math","title":"Math","text":"EPCA is an extension of PCA analogous to how generalized linear models (McCullagh and Nelder, 1989) extend linear regression. In particular, EPCA can denoise from any exponential family. Collins et al. (2001) showed that maximizing the log likelihood of any exponential family is directly related to minimizing the Bregman divergence","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"beginaligned \nB_F(p  q) equiv F(p) - F(q) - f(q)(p - q) \nendaligned","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"where ","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"beginaligned\n    f(mu) equiv nabla_mu F(mu) \n    F(mu) equiv theta cdot g(theta) - G(theta)\nendaligned","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"and g is the link function, g(theta) = nabla_theta G(theta), and mu = g(theta). In other words, F is the convex dual of cumulant \\citep{azoury2001relative}. We can now express the general formulation of the EPCA problem. For any differentiable convex function G, the EPCA problem is","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"beginaligned\n undersetThetatextminimize\n  B_Fleft(X  g(Theta) right)\nendaligned","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"where g is applied elementwise across Theta and B_F is the generalized Bregman divergence. Unfortunately, the optimal convergence constraints of the general problem remain unsolved. As such, in practice we minimize a different objective","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"beginaligned\n undersetThetatextminimize\n  B_Fleft(X  g(Theta) right) + epsilon B_Fleft(mu_0  g(Theta)right)\nendaligned","category":"page"},{"location":"math/","page":"Math","title":"Math","text":"where mu_0 in any value in mathrmrange(g) and epsilon is some small positive constant.","category":"page"},{"location":"math/#References","page":"Math","title":"References","text":"","category":"section"},{"location":"math/","page":"Math","title":"Math","text":"Collins, M.; Dasgupta, S. and Schapire, R. E. (2001). A Generalization of Principal Components Analysis to the Exponential Family. Advances in Neural Information Processing Systems 14.\n\n\n\nMcCullagh, P. and Nelder, J. A. (1989). Generalized Linear Models. 2 Edition, Chapman & Hall/CRC Monographs on Statistics and Applied                Probability (Chapman & Hall/CRC, Philadelphia, PA).\n\n\n\nPearson, K. (1901). On Lines and Planes of Closest Fit to Systems of Points in Space. Philosophical Magazine 2, 559–572.\n\n\n\n","category":"page"},{"location":"#ExpFamilyPCA.jl-Documentation","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl Documentation","text":"","category":"section"},{"location":"","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl","text":"This is some text.","category":"page"}]
}
