@article{EPCA,
  title={A Generalization of Principal Components Analysis to the Exponential Family},
  author={Collins, Michael and Dasgupta, Sanjoy and Schapire, Robert E},
  journal={Advances in Neural Information Processing Systems},
  volume={14},
  year={2001}
}

@article{azoury2001relative,
  title={Relative Loss Bounds for On-Line Density Estimation with the Exponential Family of Distributions},
  author={Azoury, Katy S and Warmuth, Manfred K},
  journal={Machine learning},
  volume={43},
  pages={211--246},
  year={2001},
  publisher={Springer}
}

@article{spectrum,
title = {Itakura-{S}aito distance based autoencoder for dimensionality reduction of mass spectra},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {167},
pages = {63-68},
year = {2017},
issn = {0169-7439},
doi = {https://doi.org/10.1016/j.chemolab.2017.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169743917300436},
author = {Yuji Nozaki and Takamichi Nakamoto},
keywords = {Dimensionality reduction, Mass spectra, Autoencoder, Itakura-Saito distance},
abstract = {Small signals may contain important information. Mass spectra of chemical compounds are usually given in a format of sparse high-dimensional data of large dynamic range. As peaks at high m/z (mass to charge ratio) region of a mass spectrum contribute to sensory information, they should not be ignored during the dimensionality reduction process even if the peak is small. However, in most of dimensionality reduction techniques, large peaks in a dataset are typically more emphasized than tiny peaks when Euclidean space is assessed. Autoencoders are widely used nonlinear dimensionality reduction technique, which is known as one special form of artificial neural networks to gain a compressed, distributed representation after learning. In this paper, we present an autoencoder which uses IS (Itakura-Saito) distance as its cost function to achieve a high capability of approximation of small target inputs in dimensionality reduction. The result of comparative experiments showed that our new autoencoder achieved the higher performance in approximation of small targets than that of the autoencoders with conventional cost functions such as the mean squared error and the cross-entropy.}
}

@misc{Julia,
      title={Julia: A Fast Dynamic Language for Technical Computing}, 
      author={Jeff Bezanson and Stefan Karpinski and Viral B. Shah and Alan Edelman},
      year={2012},
      eprint={1209.5145},
      archivePrefix={arXiv},
      primaryClass={cs.PL},
      doi={10.48550/arXiv.1209.5145},
      url={https://doi.org/10.48550/arXiv.1209.5145}
}

@BOOK{GLM,
  title     = "Generalized Linear Models",
  author    = "McCullagh, P and Nelder, John A",
  publisher = "Chapman \& Hall/CRC",
  series    = "Chapman \& Hall/CRC Monographs on Statistics and Applied
               Probability",
  edition   =  2,
  month     =  aug,
  year      =  1989,
  address   = "Philadelphia, PA",
  language  = "en"
}

@article{optim, doi = {10.21105/joss.00615}, url = {https://doi.org/10.21105/joss.00615}, year = {2018}, publisher = {The Open Journal}, volume = {3}, number = {24}, pages = {615}, author = {Patrick K. Mogensen and Asbjørn N. Riseth}, title = {Optim: A mathematical optimization package for Julia}, journal = {Journal of Open Source Software} }

@article{Bregman,
title = {The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming},
journal = {USSR Computational Mathematics and Mathematical Physics},
volume = {7},
number = {3},
pages = {200-217},
year = {1967},
issn = {0041-5553},
doi = {10.1016/0041-5553(67)90040-7},
url = {https://www.sciencedirect.com/science/article/pii/0041555367900407},
author = {L.M. Bregman},
abstract = {IN this paper we consider an iterative method of finding the common point of convex sets. This method can be regarded as a generalization of the methods discussed in [1–4]. Apart from problems which can be reduced to finding some point of the intersection of convex sets, the method considered can be applied to the approximate solution of problems in linear and convex programming.}
}

@article{symbolics,
author = {Gowda, Shashi and Ma, Yingbo and Cheli, Alessandro and Gw\'{o}\'{z}zd\'{z}, Maja and Shah, Viral B. and Edelman, Alan and Rackauckas, Christopher},
title = {High-Performance Symbolic-Numerics via Multiple Dispatch},
year = {2022},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {3},
issn = {1932-2240},
url = {https://doi.org/10.1145/3511528.3511535},
doi = {10.1145/3511528.3511535},
abstract = {As mathematical computing becomes more democratized in high-level languages, high-performance symbolic-numeric systems are necessary for domain scientists and engineers to get the best performance out of their machine without deep knowledge of code optimization. Naturally, users need different term types either to have different algebraic properties for them, or to use efficient data structures. To this end, we developed Symbolics.jl, an extendable symbolic system which uses dynamic multiple dispatch to change behavior depending on the domain needs. In this work we detail an underlying abstract term interface which allows for speed without sacrificing generality. We show that by formalizing a generic API on actions independent of implementation, we can retroactively add optimized data structures to our system without changing the pre-existing term rewriters. We showcase how this can be used to optimize term construction and give a 113x acceleration on general symbolic transformations. Further, we show that such a generic API allows for complementary term-rewriting implementations. Exploiting this feature, we demonstrate the ability to swap between classical term-rewriting simplifiers and e-graph-based term-rewriting simplifiers. We illustrate how this symbolic system improves numerical computing tasks by showcasing an e-graph ruleset which minimizes the number of CPU cycles during expression evaluation, and demonstrate how it simplifies a real-world reaction-network simulation to halve the runtime. Additionally, we show a reaction-diffusion partial differential equation solver which is able to be automatically converted into symbolic expressions via multiple dispatch tracing, which is subsequently accelerated and parallelized to give a 157x simulation speedup. Together, this presents Symbolics.jl as a next-generation symbolic-numeric computing environment geared towards modeling and simulation.},
journal = {ACM Commun. Comput. Algebra},
month = {jan},
pages = {92–96},
numpages = {5}
}

@misc{dispatch,
  author       = {Stefan Karpinski},
  title        = {The Unreasonable Effectiveness of Multiple Dispatch},
  year         = {2019},
  howpublished = {Conference Talk at JuliaCon 2019, available at \url{https://www.youtube.com/watch?v=kc9HwsxE1OY}},
  note         = {Accessed: 2024-09-13}
}

@inproceedings{ItakuraSaito,
  author    = {Itakura, Fumitada and Saito, Shuzo},
  title     = {Analysis Synthesis Telephony Based on the Maximum Likelihood Method},
  booktitle = {Proceedings of the 6th International Congress on Acoustics},
  pages     = {C--17--C--20},
  year      = {1968},
  address   = {Los Alamitos, CA},
  publisher = {IEEE}
}


@ARTICLE{simple,
  author={Li, Jun and Tao, Dacheng},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Simple Exponential Family PCA}, 
  year={2013},
  volume={24},
  number={3},
  pages={485-497},
  keywords={Principal component analysis;Bayesian methods;Computational modeling;Data models;Probabilistic logic;Complexity theory;Estimation;Automatic relevance determination;dimensionality reduction;exponential family PCA},
  doi={10.1109/TNNLS.2012.2234134}}



@article{Roy,
   title={Finding Approximate POMDP solutions Through Belief Compression},
   volume={23},
   ISSN={1076-9757},
   url={http://dx.doi.org/10.1613/jair.1496},
   DOI={10.1613/jair.1496},
   journal={Journal of Artificial Intelligence Research},
   publisher={AI Access Foundation},
   author={Roy, Nicholas and Gordon, Geoffrey and Thrun, Sebastian},
   year={2005},
   month=jan, pages={1–40} 
}

@inproceedings{shortRoy,
 author = {Roy, Nicholas and Gordon, Geoffrey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Becker and S. Thrun and K. Obermayer},
 pages = {},
 publisher = {MIT Press},
 title = {Exponential Family PCA for Belief Compression in POMDPs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/a11f9e533f28593768ebf87075ab34f2-Paper.pdf},
 volume = {15},
 year = {2002}
}


@article{PCA,
  author = {Karl Pearson},
  title = {LIII. On lines and planes of closest fit to systems of points in space },
  journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume = {2},
  number = {11},
  pages = {559--572},
  year = {1901},
  publisher = {Taylor \& Francis},
  doi = {10.1080/14786440109462720},
  URL = {https://doi.org/10.1080/14786440109462720},
}

@article{LitReview,
  title = {A Literature Review of (Sparse) Exponential Family PCA},
  volume = {16},
  ISSN = {1559-8616},
  url = {http://dx.doi.org/10.1007/s42519-021-00238-4},
  DOI = {10.1007/s42519-021-00238-4},
  number = {1},
  journal = {Journal of Statistical Theory and Practice},
  publisher = {Springer Science and Business Media LLC},
  author = {Smallman, Luke and Artemiou, Andreas},
  year = {2022},
  month = feb 
}

@misc{epca-MATLAB,
title={E-PCA},
author={Guillaume de Chambrier},
url={https://github.com/gpldecha/e-pca},
year={2016}
}