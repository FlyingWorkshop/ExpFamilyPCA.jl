@article{EPCA,
  title={A Generalization of Principal Components Analysis to the Exponential Family},
  author={Collins, Michael and Dasgupta, Sanjoy and Schapire, Robert E},
  journal={Advances in Neural Information Processing Systems},
  volume={14},
  year={2001},
  doi={10.7551/mitpress/1120.003.0084}
}

@book{elements,
  title={The {E}lements of {S}tatistical {L}earning: {D}ata {M}ining, {I}nference, and {P}rediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H and Friedman, Jerome H},
  volume={2},
  year={2009},
  publisher={Springer},
  doi={10.1007/978-0-387-84858-7}
}

@article{composition,
  title={Compositional data analysis},
  author={Greenacre, Michael},
  journal={Annual Review of Statistics and its Application},
  volume={8},
  number={1},
  pages={271--299},
  year={2021},
  publisher={Annual Reviews}
}

@misc{debiasing,
      title={Debiasing Sample Loadings and Scores in Exponential Family PCA for Sparse Count Data}, 
      author={Ruochen Huang and Yoonkyung Lee},
      year={2023},
      eprint={2312.13430},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2312.13430}, 
}

@article{topological,
  doi = {10.20382/JOCG.V9I2A6},
  url = {https://jocg.org/index.php/jocg/article/view/3066},
  author = {Edelsbrunner, Herbert and Wagner, Hubert},
  language = {en},
  title = {Topological data analysis with {B}regman divergences},
  journal = {Journal of Computational Geometry},
  pages = {Vol. 9 No. 2 (2018): Special Issue of Selected Papers from SoCG 2017},
  publisher = {Journal of Computational Geometry},
  year = {2019}
}

@misc{insurance,
      title={Compositional Data Regression in Insurance with Exponential Family PCA}, 
      author={Guojun Gan and Emiliano A. Valdez},
      year={2021},
      eprint={2112.14865},
      archivePrefix={arXiv},
      primaryClass={stat.AP},
      url={https://arxiv.org/abs/2112.14865}, 
}

@article{clustering,
  author  = {Arindam Banerjee and Srujana Merugu and Inderjit S. Dhillon and Joydeep Ghosh},
  title   = {Clustering with {B}regman Divergences},
  journal = {Journal of Machine Learning Research},
  year    = {2005},
  volume  = {6},
  number  = {58},
  pages   = {1705--1749},
  url     = {http://jmlr.org/papers/v6/banerjee05b.html}
}

@article{ultrasound,
title = {Fast reduction of speckle noise in real ultrasound images},
journal = {Signal Processing},
volume = {93},
number = {4},
pages = {684-694},
year = {2013},
issn = {0165-1684},
doi = {10.1016/j.sigpro.2012.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0165168412003258},
author = {Jie Huang and Xiaoping Yang},
keywords = {Real ultrasound image, Speckle reduction, Generalized Kullback–Leibler distance, Variable splitting, Bregman iterative},
abstract = {In this paper, we concentrate on fast removing speckle noise in real ultrasound images. It is hard to design a fast algorithm to solve a speckle reduction model, since the data fitting term in a speckle reduction model is usually not convex. In this paper, we present a convex variational model to deal with speckle noise in real ultrasound images. The data-fitting term of the proposed model is obtained by using a generalized Kullback–Leibler distance. To fast solve the proposed model, we incorporate variable splitting method and Bregman iterative method to propose a fast ultrasound speckle reduction algorithm. The capability of the proposed method is shown both on synthetic images and real ultrasound images.}
}

@article{spectrum,
title = {Itakura-{S}aito distance based autoencoder for dimensionality reduction of mass spectra},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {167},
pages = {63-68},
year = {2017},
issn = {0169-7439},
doi = {10.1016/j.chemolab.2017.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169743917300436},
author = {Yuji Nozaki and Takamichi Nakamoto},
keywords = {Dimensionality reduction, Mass spectra, Autoencoder, Itakura-Saito distance},
abstract = {Small signals may contain important information. Mass spectra of chemical compounds are usually given in a format of sparse high-dimensional data of large dynamic range. As peaks at high m/z (mass to charge ratio) region of a mass spectrum contribute to sensory information, they should not be ignored during the dimensionality reduction process even if the peak is small. However, in most of dimensionality reduction techniques, large peaks in a dataset are typically more emphasized than tiny peaks when Euclidean space is assessed. Autoencoders are widely used nonlinear dimensionality reduction technique, which is known as one special form of artificial neural networks to gain a compressed, distributed representation after learning. In this paper, we present an autoencoder which uses IS (Itakura-Saito) distance as its cost function to achieve a high capability of approximation of small target inputs in dimensionality reduction. The result of comparative experiments showed that our new autoencoder achieved the higher performance in approximation of small targets than that of the autoencoders with conventional cost functions such as the mean squared error and the cross-entropy.}
}

@article{azoury,
  title={Relative Loss Bounds for On-Line Density Estimation with the Exponential Family of Distributions},
  author={Azoury, Katy S and Warmuth, Manfred K},
  journal={Machine learning},
  volume={43},
  pages={211--246},
  year={2001},
  publisher={Springer},
  doi={10.1023/A:1010896012157}
}

@article{forster,
title = {Relative Expected Instantaneous Loss Bounds},
journal = {Journal of Computer and System Sciences},
volume = {64},
number = {1},
pages = {76-102},
year = {2002},
issn = {0022-0000},
doi = {10.1006/jcss.2001.1798},
url = {https://www.sciencedirect.com/science/article/pii/S0022000001917982},
author = {Jürgen Forster and Manfred K. Warmuth},
abstract = {In the literature a number of relative loss bounds have been shown for on-line learning algorithms. Here the relative loss is the total loss of the on-line algorithm in all trials minus the total loss of the best comparator that is chosen off-line. However, for many applications instantaneous loss bounds are more interesting where the learner first sees a batch of examples and then uses these examples to make a prediction on a new instance. We show relative expected instantaneous loss bounds for the case when the examples are i.i.d. with an unknown distribution. We bound the expected loss of the algorithm on the last example minus the expected loss of the best comparator on a random example. In particular, we study linear regression and density estimation problems and show how the leave-one-out loss can be used to prove instantaneous loss bounds for these cases. For linear regression we use an algorithm that is similar to a new on-line learning algorithm developed by Vovk. Recently a large number of relative total loss bounds have been shown that have the form O(lnT), where T is the number of trials/examples. Standard conversions of on-line algorithms to batch algorithms result in relative expected instantaneous loss bounds of the form O(lnTT). Our methods lead to O(1T) upper bounds. In many cases we give tight lower bounds.}
}

@article{gans,
	author = {Gan, Guojun and Valdez, Emiliano A.},
	journal = {Variance},
	number = {1},
	year = {2024},
	month = {may 14},
	title = {Compositional {Data} {Regression} in {Insurance} with {Exponential} {Family} {PCA}},
	volume = {17},
}


@article{Julia,
    title={Julia: A fresh approach to numerical computing},
    author={Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B},
    journal={SIAM {R}eview},
    volume={59},
    number={1},
    pages={65--98},
    year={2017},
    publisher={SIAM},
    doi={10.1137/141000671},
    url={https://epubs.siam.org/doi/10.1137/141000671}
}

@BOOK{GLM,
  title     = "Generalized {L}inear {M}odels",
  author    = "McCullagh, P and Nelder, John A",
  publisher = "Chapman \& Hall/CRC",
  series    = "Chapman \& Hall/CRC Monographs on Statistics and Applied
               Probability",
  edition   =  2,
  month     =  aug,
  year      =  1989,
  address   = "Philadelphia, PA",
  language  = "en"
}

@article{optim, doi = {10.21105/joss.00615}, url = {https://doi.org/10.21105/joss.00615}, year = {2018}, publisher = {The Open Journal}, volume = {3}, number = {24}, pages = {615}, author = {Patrick K. Mogensen and Asbjørn N. Riseth}, title = {Optim: A mathematical optimization package for {J}ulia}, journal = {Journal of Open Source Software} }

@article{Bregman,
title = {The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming},
journal = {USSR Computational Mathematics and Mathematical Physics},
volume = {7},
number = {3},
pages = {200-217},
year = {1967},
issn = {0041-5553},
doi = {10.1016/0041-5553(67)90040-7},
url = {https://www.sciencedirect.com/science/article/pii/0041555367900407},
author = {L.M. Bregman},
abstract = {IN this paper we consider an iterative method of finding the common point of convex sets. This method can be regarded as a generalization of the methods discussed in [1–4]. Apart from problems which can be reduced to finding some point of the intersection of convex sets, the method considered can be applied to the approximate solution of problems in linear and convex programming.}
}

@misc{logexp,
  title = {LogExpFunctions.jl},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/JuliaStats/LogExpFunctions.jl},
}

@unknown{stable_exp,
author = {Mächler, Martin},
year = {2015},
month = {09},
pages = {},
title = {Accurately Computing {$\log(1 - \exp(-|a|)$} Assessed by the {`Rmpfr`} package},
doi = {10.13140/RG.2.2.11834.70084}
}

@article{symbolics,
author = {Gowda, Shashi and Ma, Yingbo and Cheli, Alessandro and Gw\'{o}\'{z}zd\'{z}, Maja and Shah, Viral B. and Edelman, Alan and Rackauckas, Christopher},
title = {High-Performance Symbolic-Numerics via Multiple Dispatch},
year = {2022},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {3},
issn = {1932-2240},
url = {https://doi.org/10.1145/3511528.3511535},
doi = {10.1145/3511528.3511535},
abstract = {As mathematical computing becomes more democratized in high-level languages, high-performance symbolic-numeric systems are necessary for domain scientists and engineers to get the best performance out of their machine without deep knowledge of code optimization. Naturally, users need different term types either to have different algebraic properties for them, or to use efficient data structures. To this end, we developed Symbolics.jl, an extendable symbolic system which uses dynamic multiple dispatch to change behavior depending on the domain needs. In this work we detail an underlying abstract term interface which allows for speed without sacrificing generality. We show that by formalizing a generic API on actions independent of implementation, we can retroactively add optimized data structures to our system without changing the pre-existing term rewriters. We showcase how this can be used to optimize term construction and give a 113x acceleration on general symbolic transformations. Further, we show that such a generic API allows for complementary term-rewriting implementations. Exploiting this feature, we demonstrate the ability to swap between classical term-rewriting simplifiers and e-graph-based term-rewriting simplifiers. We illustrate how this symbolic system improves numerical computing tasks by showcasing an e-graph ruleset which minimizes the number of CPU cycles during expression evaluation, and demonstrate how it simplifies a real-world reaction-network simulation to halve the runtime. Additionally, we show a reaction-diffusion partial differential equation solver which is able to be automatically converted into symbolic expressions via multiple dispatch tracing, which is subsequently accelerated and parallelized to give a 157x simulation speedup. Together, this presents Symbolics.jl as a next-generation symbolic-numeric computing environment geared towards modeling and simulation.},
journal = {Association for Computing Machinery Communications in Computer Algebra},
month = {jan},
pages = {92–96},
numpages = {5}
}

@misc{dispatch,
  author       = {Stefan Karpinski},
  title        = {The Unreasonable Effectiveness of Multiple Dispatch},
  year         = {2019},
  howpublished = {Conference Talk at JuliaCon 2019, available at \url{https://www.youtube.com/watch?v=kc9HwsxE1OY}},
  note         = {Accessed: 2024-09-13}
}

@inproceedings{ItakuraSaito,
  author    = {Itakura, Fumitada and Saito, Shuzo},
  title     = {Analysis Synthesis Telephony Based on the Maximum Likelihood Method},
  booktitle = {Proceedings of the 6th International Congress on Acoustics},
  pages     = {C--17--C--20},
  year      = {1968},
  address   = {Los Alamitos, CA},
  publisher = {IEEE}
}


@ARTICLE{simple,
  author={Li, Jun and Tao, Dacheng},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Simple Exponential Family PCA}, 
  year={2013},
  volume={24},
  number={3},
  pages={485-497},
  keywords={Principal component analysis;Bayesian methods;Computational modeling;Data models;Probabilistic logic;Complexity theory;Estimation;Automatic relevance determination;dimensionality reduction;exponential family PCA},
  doi={10.1109/TNNLS.2012.2234134}}



@article{Roy,
   title={Finding Approximate POMDP solutions Through Belief Compression},
   volume={23},
   ISSN={1076-9757},
   url={http://dx.doi.org/10.1613/jair.1496},
   DOI={10.1613/jair.1496},
   journal={Journal of Artificial Intelligence Research},
   publisher={AI Access Foundation},
   author={Roy, Nicholas and Gordon, Geoffrey and Thrun, Sebastian},
   year={2005},
   month=jan, pages={1–40} 
}


@inproceedings{shortRoy,
 author = {Roy, Nicholas and Gordon, Geoffrey},
 booktitle = {Advances in {N}eural {I}nformation {P}rocessing {S}ystems},
 editor = {S. Becker and S. Thrun and K. Obermayer},
 pages = {},
 publisher = {MIT Press},
 title = {Exponential Family {PCA} for Belief Compression in {POMDP}s},
 url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/a11f9e533f28593768ebf87075ab34f2-Paper.pdf},
 volume = {15},
 year = {2002}
}

@article{Brad,
author = {Bradley Efron},
title = {The Estimation of Prediction Error},
journal = {Journal of the American Statistical Association},
volume = {99},
number = {467},
pages = {619--632},
year = {2004},
publisher = {ASA Website},
doi = {10.1198/016214504000000692},
URL = {https://doi.org/10.1198/016214504000000692},
}



@article{PCA1,
  author = {Karl Pearson},
  title = {On lines and planes of closest fit to systems of points in space},
  journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume = {2},
  number = {11},
  pages = {559--572},
  year = {1901},
  publisher = {Taylor \& Francis},
  doi = {10.1080/14786440109462720},
  URL = {https://doi.org/10.1080/14786440109462720},
}

@book{PCA3,
  title={Principal component analysis for special types of data},
  author={Jolliffe, Ian T},
  year={2002},
  publisher={Springer},
  doi = {10.1007/0-387-22440-8_13}
}

@article{PCA2,
  title={Analysis of a complex of statistical variables into principal components},
  author={Harold Hotelling},
  journal={Journal of Educational Psychology},
  year={1933},
  volume={24},
  pages={498-520},
  url={https://api.semanticscholar.org/CorpusID:144828484},
  doi = {10.1037/h0071325}
}

@article{LitReview,
  title = {A Literature Review of (Sparse) Exponential Family PCA},
  volume = {16},
  ISSN = {1559-8616},
  url = {http://dx.doi.org/10.1007/s42519-021-00238-4},
  DOI = {10.1007/s42519-021-00238-4},
  number = {1},
  journal = {Journal of Statistical Theory and Practice},
  publisher = {Springer Science and Business Media LLC},
  author = {Smallman, Luke and Artemiou, Andreas},
  year = {2022},
  month = feb 
}

@misc{epca-MATLAB,
title={E-PCA},
author={Guillaume de Chambrier},
url={https://github.com/gpldecha/e-pca},
year={2016}
}