@article{EPCA,
  title={A Generalization of Principal Components Analysis to the Exponential Family},
  author={Collins, Michael and Dasgupta, Sanjoy and Schapire, Robert E},
  journal={Advances in Neural Information Processing Systems},
  volume={14},
  year={2001}
}

@article{azoury2001relative,
  title={Relative Loss Bounds for On-Line Density Estimation with the Exponential Family of Distributions},
  author={Azoury, Katy S and Warmuth, Manfred K},
  journal={Machine learning},
  volume={43},
  pages={211--246},
  year={2001},
  publisher={Springer}
}

@article{spectrum,
title = {Itakura-Saito distance based autoencoder for dimensionality reduction of mass spectra},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {167},
pages = {63-68},
year = {2017},
issn = {0169-7439},
doi = {https://doi.org/10.1016/j.chemolab.2017.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169743917300436},
author = {Yuji Nozaki and Takamichi Nakamoto},
keywords = {Dimensionality reduction, Mass spectra, Autoencoder, Itakura-Saito distance},
abstract = {Small signals may contain important information. Mass spectra of chemical compounds are usually given in a format of sparse high-dimensional data of large dynamic range. As peaks at high m/z (mass to charge ratio) region of a mass spectrum contribute to sensory information, they should not be ignored during the dimensionality reduction process even if the peak is small. However, in most of dimensionality reduction techniques, large peaks in a dataset are typically more emphasized than tiny peaks when Euclidean space is assessed. Autoencoders are widely used nonlinear dimensionality reduction technique, which is known as one special form of artificial neural networks to gain a compressed, distributed representation after learning. In this paper, we present an autoencoder which uses IS (Itakura-Saito) distance as its cost function to achieve a high capability of approximation of small target inputs in dimensionality reduction. The result of comparative experiments showed that our new autoencoder achieved the higher performance in approximation of small targets than that of the autoencoders with conventional cost functions such as the mean squared error and the cross-entropy.}
}

@misc{Julia,
      title={Julia: A Fast Dynamic Language for Technical Computing}, 
      author={Jeff Bezanson and Stefan Karpinski and Viral B. Shah and Alan Edelman},
      year={2012},
      eprint={1209.5145},
      archivePrefix={arXiv},
      primaryClass={cs.PL},
      doi={10.48550/arXiv.1209.5145},
      url={https://doi.org/10.48550/arXiv.1209.5145}
}

@BOOK{GLM,
  title     = "Generalized Linear Models",
  author    = "McCullagh, P and Nelder, John A",
  publisher = "Chapman \& Hall/CRC",
  series    = "Chapman \& Hall/CRC Monographs on Statistics and Applied
               Probability",
  edition   =  2,
  month     =  aug,
  year      =  1989,
  address   = "Philadelphia, PA",
  language  = "en"
}

@inproceedings{ItakuraSaito,
  author    = {Itakura, Fumitada and Saito, Shuzo},
  title     = {Analysis Synthesis Telephony Based on the Maximum Likelihood Method},
  booktitle = {Proceedings of the 6th International Congress on Acoustics},
  pages     = {C--17--C--20},
  year      = {1968},
  address   = {Los Alamitos, CA},
  publisher = {IEEE}
}


@ARTICLE{simple,
  author={Li, Jun and Tao, Dacheng},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Simple Exponential Family PCA}, 
  year={2013},
  volume={24},
  number={3},
  pages={485-497},
  keywords={Principal component analysis;Bayesian methods;Computational modeling;Data models;Probabilistic logic;Complexity theory;Estimation;Automatic relevance determination;dimensionality reduction;exponential family PCA},
  doi={10.1109/TNNLS.2012.2234134}}



@article{Roy,
   title={Finding Approximate POMDP solutions Through Belief Compression},
   volume={23},
   ISSN={1076-9757},
   url={http://dx.doi.org/10.1613/jair.1496},
   DOI={10.1613/jair.1496},
   journal={Journal of Artificial Intelligence Research},
   publisher={AI Access Foundation},
   author={Roy, N. and Gordon, G. and Thrun, S.},
   year={2005},
   month=jan, pages={1â€“40} 
}


@article{PCA,
  author = {Karl Pearson},
  title = {LIII. On lines and planes of closest fit to systems of points in space },
  journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume = {2},
  number = {11},
  pages = {559--572},
  year = {1901},
  publisher = {Taylor \& Francis},
  doi = {10.1080/14786440109462720},
  URL = {https://doi.org/10.1080/14786440109462720},
}

@article{LitReview,
  title = {A Literature Review of (Sparse) Exponential Family PCA},
  volume = {16},
  ISSN = {1559-8616},
  url = {http://dx.doi.org/10.1007/s42519-021-00238-4},
  DOI = {10.1007/s42519-021-00238-4},
  number = {1},
  journal = {Journal of Statistical Theory and Practice},
  publisher = {Springer Science and Business Media LLC},
  author = {Smallman, Luke and Artemiou, Andreas},
  year = {2022},
  month = feb 
}

@misc{epca-MATLAB,
title={E-PCA},
author={Guillaume de Chambrier},
url={https://github.com/gpldecha/e-pca},
year={2016}
}